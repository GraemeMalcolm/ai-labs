[
  {
    "id": 84,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Image classification",
    "content": "One of the oldest computer vision solutions is a technique called *image classification*, in which a model that has been trained with a large number of images is used to predict a text label based on an image's contents.\n\nFor example, suppose a grocery store wants to implement smart checkout system that identifies produce automatically. For example, the customer could place fruits or vegetables on a scale at the checkout, and an AI application connected to a camera could automatically identify t",
    "summary": "One of the oldest computer vision solutions is a technique called *image classification*, in which a model that has been trained with a large number of images is used to predict a text label based on...",
    "keywords": [
      "smart",
      "charge",
      "weight",
      "photographs",
      "orange",
      "model",
      "correct",
      "main",
      "features",
      "technique",
      "identifies",
      "place",
      "name",
      "grocery",
      "based",
      "subject",
      "solution",
      "visual",
      "media",
      "automatically"
    ]
  },
  {
    "id": 85,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Object detection",
    "content": "Suppose the grocery store wants a more sophisticated system, in which the checkout can scan multiple items on the checkout and identify each of them. A common approach to this type of problem is called \"object detection\". Object detection models examine multiple regions in an image to find individual objects and their locations. The resulting prediction from the model includes which objects were detected, and the specific regions of the image in which they appear - indicated by the coordinates o",
    "summary": "Suppose the grocery store wants a more sophisticated system, in which the checkout can scan multiple items on the checkout and identify each of them. A common approach to this type of problem is calle...",
    "keywords": [
      "object-detection",
      "they",
      "orange",
      "model",
      "find",
      "models",
      "detected",
      "grocery",
      "objects",
      "scan",
      "more",
      "media",
      "coordinates",
      "indicated",
      "them",
      "locations",
      "resulting",
      "checkout",
      "detection",
      "includes"
    ]
  },
  {
    "id": 86,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Semantic segmentation",
    "content": "Another, more sophisticated way to detect objects in an image, is called \"semantic segmentation\". In this approach, a model is trained to find objects, and classify individual pixels in the image based on the object to which they belong. The result of this process is a much more precise prediction of the location of objects in the image.\n\n![Photograph of an orange, apple, and banana with overlaid masks.](../media/semantic-segmentation.png)",
    "summary": "Another, more sophisticated way to detect objects in an image, is called \"semantic segmentation\". In this approach, a model is trained to find objects, and classify individual pixels in the image base...",
    "keywords": [
      "they",
      "orange",
      "model",
      "location",
      "find",
      "objects",
      "based",
      "more",
      "media",
      "overlaid",
      "process",
      "photograph",
      "pixels",
      "approach",
      "precise",
      "much",
      "result",
      "semantic-segmentation",
      "detect",
      "sophisticated"
    ]
  },
  {
    "id": 87,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Contextual image analysis",
    "content": "The latest *multimodal* computer vision models are trained to find contextual relationships between objects in images and the text that describes them. The result is an ability to semantically interpret an image to determine what objects and activities it depicts; and generate appropriate descriptions or suggest relevant tags.\n\n![Photograph of a person eating an apple.](../media/image-analysis.png)\n\n***A person eating an apple.***",
    "summary": "The latest *multimodal* computer vision models are trained to find contextual relationships between objects in images and the text that describes them. The result is an ability to semantically interpr...",
    "keywords": [
      "contextual",
      "semantically",
      "suggest",
      "find",
      "models",
      "analysis",
      "objects",
      "latest",
      "person",
      "interpret",
      "media",
      "them",
      "generate",
      "activities",
      "vision",
      "photograph",
      "between",
      "eating",
      "result",
      "relationships"
    ]
  },
  {
    "id": 88,
    "category": "Computer Vision",
    "file": "02-understand-image-processing.md",
    "heading": "Filters",
    "content": "A common way to perform image processing tasks is to apply *filters* that modify the pixel values of the image to create a visual effect. A filter is defined by one or more arrays of pixel values, called filter *kernels*. For example, you could define filter with a 3x3 kernel as shown in this example:\n\n```\n-1 -1 -1\n-1  8 -1\n-1 -1 -1\n```\n\nThe kernel is then *convolved* across the image, calculating a weighted sum for each 3x3 patch of pixels and assigning the result to a new image. It's easier to",
    "summary": "A common way to perform image processing tasks is to apply *filters* that modify the pixel values of the image to create a visual effect. A filter is defined by one or more arrays of pixel values, cal...",
    "keywords": [
      "grayscale",
      "entire",
      "filtered",
      "animation",
      "objects",
      "diagram",
      "same",
      "dog-grayscale",
      "process",
      "edge",
      "common",
      "values",
      "operation",
      "clearly",
      "again",
      "kinds",
      "manipulation",
      "convolved",
      "which",
      "apply"
    ]
  },
  {
    "id": 89,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Semantic modeling for language - Transformers",
    "content": "Transformers work by processing huge volumes of data, and encoding language *tokens* (representing individual words or phrases) as vector-based *embeddings* (arrays of numeric values). A technique called *attention* is used to assign embedding values that reflect different aspects of how each token is used in the context of other tokens. You can think of the embeddings as vectors in multidimensional space, in which each dimension embeds a linguistic attribute of a token based on its context in t",
    "summary": "Transformers work by processing huge volumes of data, and encoding language *tokens* (representing individual words or phrases) as vector-based *embeddings* (arrays of numeric values). A technique cal...",
    "keywords": [
      "arrays",
      "tokens",
      "semantically",
      "model",
      "translation",
      "technique",
      "analysis",
      "data",
      "based",
      "diagram",
      "contexts",
      "language",
      "commonly",
      "more",
      "assign",
      "embeddings",
      "phrases",
      "numeric",
      "define",
      "work"
    ]
  },
  {
    "id": 90,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Semantic model for images - Vision transformers",
    "content": "The success of transformers as a way to build language models has led AI researchers to consider whether the same approach would be effective for image data. The result is the development of *vision transformer* (ViT) models, in which a model is trained using a large volume of images. Instead of encoding text-based tokens, the transformer extracts *patches* of pixel values from the image, and generates a linear vector from the pixel values.\n\n![Diagram of a photo with patches assigned to vectors.",
    "summary": "The success of transformers as a way to build language models has led AI researchers to consider whether the same approach would be effective for image data. The result is the development of *vision t...",
    "keywords": [
      "development",
      "characteristics",
      "diagram",
      "same",
      "using",
      "embeddings",
      "approach",
      "common",
      "values",
      "attention",
      "similar",
      "which",
      "directions",
      "patches",
      "shape",
      "used",
      "context",
      "images",
      "linear",
      "media"
    ]
  },
  {
    "id": 91,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Bringing it all together - Multimodal models",
    "content": "A language transformer creates embeddings that define a linguistic vocabulary that encode semantic relationships between words. A vision transformer creates a visual vocabulary that does the same for visual features. When the training data includes images with associated text descriptions, we can combine the encoders from both of these transformers in a *multimodal* model; and use a technique called *cross-model attention* to define a unified spatial representation of the embeddings, like this.\n",
    "summary": "A language transformer creates embeddings that define a linguistic vocabulary that encode semantic relationships between words. A vision transformer creates a visual vocabulary that does the same for...",
    "keywords": [
      "model",
      "features",
      "models",
      "bringing",
      "technique",
      "data",
      "complex",
      "person",
      "backpack",
      "diagram",
      "language",
      "same",
      "visual",
      "spatial",
      "creates",
      "previously",
      "vector",
      "embeddings",
      "image-description",
      "vision"
    ]
  }
]