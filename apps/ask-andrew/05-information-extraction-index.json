[
  {
    "id": 92,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Financial document processing",
    "content": "**Invoice processing** solutions can analyze invoices to extract:\n\n- **Vendor information**: Company names, addresses, and contact details.\n- **Transaction details**: Invoice numbers, dates, and payment terms.\n- **Line items**: Product descriptions, quantities, unit prices, and totals.\n- **Tax information**: Tax rates, amounts, and exempt items.\n\n**Receipt processing** solutions might need to read receipts to extract:\n\n- **Merchant details**: Store names, locations, and transaction IDs.\n- **Purc",
    "summary": "**Invoice processing** solutions can analyze invoices to extract:\n\n- **Vendor information**: Company names, addresses, and contact details.\n- **Transaction details**: Invoice numbers, dates, and payme...",
    "keywords": [
      "reporting",
      "prices",
      "statements",
      "methods",
      "exempt",
      "merchant",
      "invoice",
      "addresses",
      "data",
      "payment",
      "profit",
      "transaction",
      "balances",
      "quantities",
      "invoices",
      "totals",
      "receipt",
      "locations",
      "receipts",
      "terms"
    ]
  },
  {
    "id": 93,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Legal and compliance documents",
    "content": "**Contract processing** solutions can be used to extract:\n\n- **Party information**: Contracting parties, signatories, and witnesses.\n- **Terms and conditions**: Effective dates, renewal terms, and termination clauses.\n- **Financial terms**: Payment schedules, penalties, and insurance requirements.\n\n**Regulatory forms** that may need to be processed include:\n\n- **Tax documents**: W-2s, 1099s, and other tax forms.\n- **Insurance forms**: Policy numbers, claim amounts, and incident details.\n- **Gove",
    "summary": "**Contract processing** solutions can be used to extract:\n\n- **Party information**: Contracting parties, signatories, and witnesses.\n- **Terms and conditions**: Effective dates, renewal terms, and ter...",
    "keywords": [
      "government",
      "renewal",
      "forms",
      "termination",
      "data",
      "payment",
      "contract",
      "legal",
      "terms",
      "party",
      "certification",
      "regulatory",
      "application",
      "clauses",
      "witnesses",
      "include",
      "need",
      "extract",
      "effective",
      "insurance"
    ]
  },
  {
    "id": 94,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Healthcare documentation",
    "content": "**Medical records** can be processed to retrieve:\n\n- **Patient information**: Demographics, medical record numbers, and insurance details.\n- **Clinical data**: Diagnoses, treatments, medication lists, and vital signs.\n- **Administrative data**: Appointment schedules, billing codes, and provider information.",
    "summary": "**Medical records** can be processed to retrieve:\n\n- **Patient information**: Demographics, medical record numbers, and insurance details.\n- **Clinical data**: Diagnoses, treatments, medication lists,...",
    "keywords": [
      "records",
      "appointment",
      "data",
      "medication",
      "documentation",
      "provider",
      "record",
      "healthcare",
      "treatments",
      "codes",
      "administrative",
      "vital",
      "clinical",
      "insurance",
      "details",
      "signs",
      "patient",
      "diagnoses",
      "medical",
      "retrieve"
    ]
  },
  {
    "id": 95,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Supply chain and logistics",
    "content": "**Shipping documents** often contain vital details such as:\n\n- **Shipment details**: Tracking numbers, weights, and dimensions.\n- **Address information**: Sender and recipient details, and delivery instructions.\n- **Customs documentation**: Commodity codes, values, and country of origin.\n\n**Purchase Orders** are commonly processed to extract:\n\n- **Vendor information**: Supplier details and contact information.\n- **Product specifications**: Item codes, descriptions, and quantities.\n- **Delivery r",
    "summary": "**Shipping documents** often contain vital details such as:\n\n- **Shipment details**: Tracking numbers, weights, and dimensions.\n- **Address information**: Sender and recipient details, and delivery in...",
    "keywords": [
      "contain",
      "systems",
      "dimensions",
      "recipient",
      "shipment",
      "documentation",
      "item",
      "delivery",
      "commonly",
      "special",
      "more",
      "quantities",
      "locations",
      "using",
      "sender",
      "commodity",
      "values",
      "specifications",
      "vendor",
      "chain"
    ]
  },
  {
    "id": 96,
    "category": "Information Extraction",
    "file": "02-overview.md",
    "heading": "Choosing the right approach",
    "content": "When planning an information extraction solution, it's important to consider the requirements and constraints that the system must address. Some key considerations include:\n\n- **Document characteristics**. The documents from which you need to extract data are the basis of the whole solution. Consider factors like:\n    - **Layout consistency**: Standardized forms favor template-based approaches, while a need to process multiple formats and layouts may require a more complex machine learning based",
    "summary": "When planning an information extraction solution, it's important to consider the requirements and constraints that the system must address. Some key considerations include:\n\n- **Document characteristi...",
    "keywords": [
      "standardized",
      "characteristics",
      "deep",
      "complex",
      "process",
      "approach",
      "offer",
      "security",
      "require",
      "template-based",
      "variable",
      "need",
      "latency",
      "access",
      "compatibility",
      "sensitive",
      "choosing",
      "which",
      "infrastructure",
      "document"
    ]
  },
  {
    "id": 97,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "The OCR pipeline: A step-by-step process",
    "content": "The OCR pipeline consists of five essential stages that work together to transform visual information into text data.\n\n![Diagram of the OCR pipeline.](../media/optical-character-recognition.png)\n\nThe stages in the OCR process are:\n\n1. Image acquisition and input.\n1. Preprocessing and image enhancement.\n1. Text region detection.\n1. Character recognition and classification.\n1. Output generation and post-processing.\n\nLet's examine each stage in more depth.",
    "summary": "The OCR pipeline consists of five essential stages that work together to transform visual information into text data.\n\n![Diagram of the OCR pipeline.](../media/optical-character-recognition.png)\n\nThe...",
    "keywords": [
      "data",
      "diagram",
      "more",
      "step-by-step",
      "visual",
      "process",
      "input",
      "detection",
      "work",
      "output",
      "depth",
      "consists",
      "stage",
      "enhancement",
      "region",
      "stages",
      "generation",
      "examine",
      "acquisition",
      "optical-character-recognition"
    ]
  },
  {
    "id": 98,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 1: Image acquisition and input",
    "content": "The pipeline begins when an image containing text enters the system. This could be:\n\n- A photograph taken with a smartphone camera.\n- A scanned document from a flatbed or document scanner.\n- A frame extracted from a video stream.\n- A PDF page rendered as an image.",
    "summary": "The pipeline begins when an image containing text enters the system. This could be:\n\n- A photograph taken with a smartphone camera.\n- A scanned document from a flatbed or document scanner.\n- A frame e...",
    "keywords": [
      "containing",
      "page",
      "scanned",
      "input",
      "photograph",
      "frame",
      "system",
      "extracted",
      "stream",
      "stage",
      "rendered",
      "flatbed",
      "begins",
      "smartphone",
      "acquisition",
      "document",
      "camera",
      "video",
      "text",
      "scanner"
    ]
  },
  {
    "id": 99,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 2: Preprocessing and image enhancement",
    "content": "Before text detection begins, the following techniques are used to optimize the image for better recognition accuracy:\n\n- **Noise reduction** removes visual artifacts, dust spots, and scanning imperfections that could interfere with text detection. The specific techniques used to perform noise reduction include:\n\n    - **Filtering and image processing algorithms**: Gaussian filters, median filters, and morphological operations.\n    - **Machine learning models**: Denoising autoencoders and convol",
    "summary": "Before text detection begins, the following techniques are used to optimize the image for better recognition accuracy:\n\n- **Noise reduction** removes visual artifacts, dust spots, and scanning imperfe...",
    "keywords": [
      "interpolation",
      "corrects",
      "spots",
      "methods",
      "deep",
      "scanning",
      "algorithms",
      "rotation",
      "connected",
      "level",
      "correction",
      "adaptive",
      "again",
      "cnns",
      "imperfections",
      "types",
      "enhancement",
      "bilinear",
      "adjusts",
      "gaussian"
    ]
  },
  {
    "id": 100,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 3: Text region detection",
    "content": "The system analyzes the preprocessed image to identify areas that contain text by using the following techniques:\n\n- **Layout analysis** distinguishes between text regions, images, graphics, and white space areas. Techniques for layout analysis include:\n\n    - **Traditional approaches**: Connected component analysis, run-length encoding, and projection-based segmentation.\n    - **Deep learning models**: Semantic segmentation networks like U-Net, Mask R-CNN, and specialized document layout analys",
    "summary": "The system analyzes the preprocessed image to identify areas that contain text by using the following techniques:\n\n- **Layout analysis** distinguishes between text regions, images, graphics, and white...",
    "keywords": [
      "determined",
      "methods",
      "layoutlm",
      "deep",
      "support",
      "algorithms",
      "structure",
      "r-cnn",
      "formatting",
      "using",
      "connected",
      "common",
      "headers",
      "groups",
      "types",
      "region",
      "mask",
      "u-net",
      "publaynet-trained",
      "projection-based"
    ]
  },
  {
    "id": 101,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 4: Character recognition and classification",
    "content": "This is the core of the OCR process where individual characters are identified:\n\n- **Feature extraction**: Analyzes the shape, size, and distinctive characteristics of each character or symbol.\n    - **Traditional methods**: Statistical features like moments, Fourier descriptors, and structural features (loops, endpoints, intersections)\n    - **Deep learning approaches**: Convolutional neural networks that automatically learn discriminative features from raw pixel data\n\n- **Pattern matching**: C",
    "summary": "This is the core of the OCR process where individual characters are identified:\n\n- **Feature extraction**: Analyzes the shape, size, and distinctive characteristics of each character or symbol.\n    -...",
    "keywords": [
      "lstm",
      "methods",
      "characteristics",
      "deep",
      "support",
      "comparison",
      "descriptors",
      "recognized",
      "resnet",
      "algorithms",
      "process",
      "making",
      "automatically",
      "using",
      "correction",
      "cnns",
      "scoring",
      "attention",
      "combining",
      "n-gram"
    ]
  },
  {
    "id": 102,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 5: Output generation and post-processing",
    "content": "The final stage converts recognition results into usable text data:\n\n- **Text compilation**: Assembles individual character recognitions into complete words and sentences.\n    - **Rule-based assembly**: Deterministic algorithms that combine character predictions using spatial proximity and confidence thresholds.\n    - **Sequence models**: Recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks that model text as sequential data.\n    - **Attention-based models**: Transformer a",
    "summary": "The final stage converts recognition results into usable text data:\n\n- **Text compilation**: Assembles individual character recognitions into complete words and sentences.\n    - **Rule-based assembly*...",
    "keywords": [
      "lstm",
      "layoutlm",
      "coordinate",
      "positioning",
      "complex",
      "comprehensive",
      "algorithms",
      "structure",
      "domain",
      "using",
      "precise",
      "correction",
      "error",
      "thresholds",
      "long",
      "efficient",
      "exact",
      "generation",
      "sentences",
      "combining"
    ]
  },
  {
    "id": 103,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "The field extraction pipeline",
    "content": "Field extraction follows a systematic pipeline that transforms OCR output into structured data.\n\n![Diagram of the field extraction pipeline.](../media/field-extraction.png)\n\nThe stages in the field extraction process are:\n\n1. OCR output ingestion.\n1. Field detection and candidate identification.\n1. Field mapping and association.\n1. Data normalization and standardization.\n1. Integration with business processes and systems.\n\nLet's explore these stages in more detail.",
    "summary": "Field extraction follows a systematic pipeline that transforms OCR output into structured data.\n\n![Diagram of the field extraction pipeline.](../media/field-extraction.png)\n\nThe stages in the field ex...",
    "keywords": [
      "systems",
      "ingestion",
      "identification",
      "mapping",
      "association",
      "data",
      "diagram",
      "more",
      "field-extraction",
      "process",
      "detection",
      "candidate",
      "output",
      "detail",
      "explore",
      "processes",
      "normalization",
      "stages",
      "structured",
      "transforms"
    ]
  },
  {
    "id": 104,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 1: OCR output ingestion",
    "content": "The process begins with the structured output from the OCR pipeline, which can include:\n\n- **Raw text content**: The actual characters and words extracted from the document\n- **Positional metadata**: Bounding box coordinates, page locations, and reading order information\n- **Confidence scores**: OCR engine confidence levels for each text element\n- **Layout information**: Document structure, line breaks, paragraph boundaries",
    "summary": "The process begins with the structured output from the OCR pipeline, which can include:\n\n- **Raw text content**: The actual characters and words extracted from the document\n- **Positional metadata**:...",
    "keywords": [
      "ingestion",
      "page",
      "boundaries",
      "scores",
      "structure",
      "coordinates",
      "process",
      "locations",
      "output",
      "order",
      "paragraph",
      "extracted",
      "characters",
      "include",
      "confidence",
      "stage",
      "actual",
      "line",
      "breaks",
      "begins"
    ]
  },
  {
    "id": 105,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 2: Field detection and candidate identification",
    "content": "This stage identifies potential field value in the OCR output. There are multiple approaches that can be used, independently or in combination, to determine the likely fields in the OCR results.",
    "summary": "This stage identifies potential field value in the OCR output. There are multiple approaches that can be used, independently or in combination, to determine the likely fields in the OCR results.",
    "keywords": [
      "results",
      "identification",
      "identifies",
      "detection",
      "candidate",
      "output",
      "stage",
      "combination",
      "fields",
      "used",
      "potential",
      "there",
      "multiple",
      "approaches",
      "determine",
      "likely",
      "value",
      "independently",
      "field"
    ]
  },
  {
    "id": 106,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Template-based detection",
    "content": "Templates for field detection rely on rule-based pattern matching. Field identification may be accomplished using techniques such as:\n\n- Predefined document layouts with known field positions and anchor keywords.\n- Searches for label-value pairs like \"Invoice Number:\", \"Date:\", \"Total:\".\n- Regular expressions and string matching algorithms.\n\n**Advantages** of a template-based approach include high accuracy for known document types, fast processing, and explainable results.\n\n**Limitations** of th",
    "summary": "Templates for field detection rely on rule-based pattern matching. Field identification may be accomplished using techniques such as:\n\n- Predefined document layouts with known field positions and anch...",
    "keywords": [
      "keywords",
      "results",
      "inconsistencies",
      "matching",
      "explainable",
      "identification",
      "rely",
      "accomplished",
      "string",
      "invoice",
      "pattern",
      "algorithms",
      "accuracy",
      "using",
      "template",
      "detection",
      "creation",
      "approach",
      "like",
      "limitations"
    ]
  },
  {
    "id": 107,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Machine learning-based detection",
    "content": "Instead of hard-coded logic to extract fields based on known names and locations, you can use a corpus of example documents to train a machine learning model that extracts the fields based on learned relationships. *Transformer*-based models in particular are good at applying contextual cues to identify patterns, and so are often the basis of a field detection solution.\n\nTraining approaches for field detection machine learning models include:\n\n- **Supervised learning**: Trained on labeled datase",
    "summary": "Instead of hard-coded logic to extract fields based on known names and locations, you can use a corpus of example documents to train a machine learning model that extracts the fields based on learned...",
    "keywords": [
      "corpus",
      "self-supervised",
      "connections",
      "values",
      "attention",
      "identify",
      "document",
      "corpora",
      "focus",
      "patterns",
      "assignments",
      "neural",
      "good",
      "when",
      "mechanisms",
      "field",
      "contextual",
      "applying",
      "model",
      "based"
    ]
  },
  {
    "id": 108,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Generative AI for schema-based extraction",
    "content": "Recent advances in large language models (LLMs) have led to the emergence of generative AI-based field detection techniques, which enable more efficient and effective field detection through:\n\n- **Prompt-based extraction** in which you provide the LLM with document text and a schema definition, and it matches the text to the fields in the schema.\n- **Few-shot learning** in which you can train models with minimal examples to extract custom fields.\n- **Chain-of-thought reasoning** that guides mode",
    "summary": "Recent advances in large language models (LLMs) have led to the emergence of generative AI-based field detection techniques, which enable more efficient and effective field detection through:\n\n- **Pro...",
    "keywords": [
      "examples",
      "schema-based",
      "definition",
      "prompt-based",
      "identification",
      "models",
      "language",
      "advances",
      "more",
      "minimal",
      "step-by-step",
      "custom",
      "detection",
      "enable",
      "guides",
      "provide",
      "generative",
      "efficient",
      "chain-of-thought",
      "extract"
    ]
  },
  {
    "id": 109,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 3: Field mapping and association",
    "content": "After candidate values are identified, they must be mapped to specific schema fields:",
    "summary": "After candidate values are identified, they must be mapped to specific schema fields:",
    "keywords": [
      "fields",
      "they",
      "candidate",
      "values",
      "after",
      "mapping",
      "association",
      "identified",
      "schema",
      "stage",
      "mapped",
      "field",
      "specific"
    ]
  },
  {
    "id": 110,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Key-value pairing techniques",
    "content": "In many cases, data fields in a document or form are discrete values that can be mapped to keys - for example, the vendor name, date, and total amount in a receipt or invoice. Common techniques used for key-value pairing include:\n\n- **Proximity analysis**:\n    - **Spatial clustering**: Group nearby text elements using distance algorithms.\n    - **Reading order analysis**: Follow natural text flow to associate labels with values.\n    - **Geometric relationships**: Use alignment, indentation, and ",
    "summary": "In many cases, data fields in a document or form are discrete values that can be mapped to keys - for example, the vendor name, date, and total amount in a receipt or invoice. Common techniques used f...",
    "keywords": [
      "keys",
      "geometric",
      "name",
      "analysis",
      "invoice",
      "data",
      "alignment",
      "positioning",
      "pattern",
      "understand",
      "analyze",
      "algorithms",
      "spatial",
      "receipt",
      "using",
      "discrete",
      "common",
      "natural",
      "values",
      "example"
    ]
  },
  {
    "id": 111,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Table and structured content processing",
    "content": "Some documents include more complex structures of text, such as tables. For example, a receipt or invoice might include a table of line items with columns for the item name, price, and the quantity purchased.\n\nThe presence of a table can be determined using several techniques, including:\n\n- Specialized convolutional neural network (CNN) architectures for table structure recognition.\n- Object detection approaches adapted for table cell identification.\n- Graph-based parsing approaches that model t",
    "summary": "Some documents include more complex structures of text, such as tables. For example, a receipt or invoice might include a table of line items with columns for the item name, price, and the quantity pu...",
    "keywords": [
      "determined",
      "model",
      "identification",
      "adapted",
      "name",
      "association",
      "invoice",
      "complex",
      "item",
      "understand",
      "convolutional",
      "more",
      "solution",
      "structure",
      "several",
      "cell",
      "receipt",
      "using",
      "table",
      "columns"
    ]
  },
  {
    "id": 112,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Confidence scoring and validation",
    "content": "Field detection and extraction accuracy depends on many factors, and the algorithms and models used to implement the solution are subject to potential misidentification or value interpretation errors. To account for this, various techniques are employed to evaluate the accuracy of the predicted field values; including:\n\n- **OCR confidence**: Inheriting confidence scores from the underlying text recognition.\n- **Pattern matching confidence**: Scoring based on how well extraction matches expected ",
    "summary": "Field detection and extraction accuracy depends on many factors, and the algorithms and models used to implement the solution are subject to potential misidentification or value interpretation errors....",
    "keywords": [
      "matching",
      "depends",
      "models",
      "invoice",
      "validation",
      "pattern",
      "cross-field",
      "based",
      "item",
      "subject",
      "scores",
      "solution",
      "algorithms",
      "accuracy",
      "well",
      "including",
      "detection",
      "values",
      "example",
      "between"
    ]
  },
  {
    "id": 113,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 4: Data normalization and standardization",
    "content": "Raw extracted values are generally transformed into consistent formats (for example to ensure that all extracted dates are expressed in the same date format) and checked for validity.",
    "summary": "Raw extracted values are generally transformed into consistent formats (for example to ensure that all extracted dates are expressed in the same date format) and checked for validity.",
    "keywords": [
      "data",
      "same",
      "checked",
      "transformed",
      "values",
      "example",
      "format",
      "validity",
      "extracted",
      "consistent",
      "ensure",
      "stage",
      "normalization",
      "formats",
      "standardization",
      "into",
      "dates",
      "expressed",
      "generally",
      "date"
    ]
  },
  {
    "id": 114,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Format standardization",
    "content": "Examples of format standardization that can be implemented include:\n\n- **Date normalization**:\n    - **Format detection**: Identify various date formats (MM/DD/YYYY, DD-MM-YYYY, etc.).\n    - **Parsing algorithms**: Convert to standardized ISO formats.\n    - **Ambiguity resolution**: Handle cases where date format is unclear.\n\n- **Currency and numeric processing**:\n    - **Symbol recognition**: Handle different currency symbols and thousand separators.\n    - **Decimal normalization**: Standardize",
    "summary": "Examples of format standardization that can be implemented include:\n\n- **Date normalization**:\n    - **Format detection**: Identify various date formats (MM/DD/YYYY, DD-MM-YYYY, etc.).\n    - **Parsing...",
    "keywords": [
      "examples",
      "abbreviation",
      "case",
      "standardized",
      "forms",
      "dd-mm-yyyy",
      "implemented",
      "unclear",
      "algorithms",
      "special",
      "needed",
      "point",
      "resolution",
      "numeric",
      "detection",
      "decimal",
      "standardize",
      "representation",
      "format",
      "common"
    ]
  },
  {
    "id": 115,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Data Validation and Quality Assurance",
    "content": "As well as formatting the extracted fields, the standardization process enables further validation of the values that have been extracted through techniques like:\n\n- **Rule-based validation**:\n    - **Format checking**: Verify extracted values match expected patterns (phone numbers, email addresses).\n    - **Range validation**: Ensure numeric values fall within reasonable bounds.\n    - **Required field checking**: Confirm all mandatory fields are present.\n\n- **Statistical validation**:\n    - **O",
    "summary": "As well as formatting the extracted fields, the standardization process enables further validation of the values that have been extracted through techniques like:\n\n- **Rule-based validation**:\n    - *...",
    "keywords": [
      "fall",
      "reasonable",
      "analysis",
      "validation",
      "data",
      "addresses",
      "assurance",
      "process",
      "formatting",
      "unusually",
      "related",
      "well",
      "numeric",
      "within",
      "required",
      "detection",
      "like",
      "values",
      "format",
      "cross-document"
    ]
  },
  {
    "id": 116,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 5: Integration with business processes and systems",
    "content": "The final stage of the process usually involves integrating the extracted field values into a business process or system:",
    "summary": "The final stage of the process usually involves integrating the extracted field values into a business process or system:",
    "keywords": [
      "into",
      "integration",
      "systems",
      "final",
      "values",
      "processes",
      "usually",
      "involves",
      "extracted",
      "system",
      "stage",
      "integrating",
      "field",
      "business",
      "process"
    ]
  },
  {
    "id": 117,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Schema mapping",
    "content": "The extracted fields may need to be further transformed or reformatted so they align with application schemas used for data ingestion into downstream systems. For example:\n\n- **Database schemas**: Map extracted fields to specific database columns and tables.\n- **API payloads**: Format data for REST API consumption by downstream systems.\n- **Message queues**: Prepare structured messages for asynchronous processing.\n\nThe schema mapping process might involve transformations such as:\n\n- **Field rena",
    "summary": "The extracted fields may need to be further transformed or reformatted so they align with application schemas used for data ingestion into downstream systems. For example:\n\n- **Database schemas**: Map...",
    "keywords": [
      "they",
      "systems",
      "ingestion",
      "mapping",
      "asynchronous",
      "data",
      "conditional",
      "reformatted",
      "process",
      "transformed",
      "columns",
      "involve",
      "derivation",
      "example",
      "schemas",
      "format",
      "values",
      "application",
      "extracted",
      "system"
    ]
  },
  {
    "id": 118,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Quality metrics and reporting",
    "content": "Another common task after the extraction process has finished is to evaluate and report on the quality of the extracted data. The report can include information such as:\n\n- **Field-level confidence scores**: Individual confidence ratings for each extracted field.\n- **Document-level quality assessment**: Overall extraction success metrics.\n- **Error categorization**: Classify extraction failures by type and cause.",
    "summary": "Another common task after the extraction process has finished is to evaluate and report on the quality of the extracted data. The report can include information such as:\n\n- **Field-level confidence sc...",
    "keywords": [
      "field-level",
      "after",
      "failures",
      "field",
      "data",
      "scores",
      "ratings",
      "process",
      "document-level",
      "common",
      "finished",
      "error",
      "extracted",
      "include",
      "confidence",
      "categorization",
      "cause",
      "metrics",
      "type",
      "such"
    ]
  }
]