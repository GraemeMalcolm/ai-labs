[
  {
    "id": 47,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Common speech recognition scenarios",
    "content": "Speech recognition, also called speech-to-text, listens to audio input and transcribes it into written text. This capability powers a wide range of business and consumer applications.",
    "summary": "Speech recognition, also called speech-to-text, listens to audio input and transcribes it into written text. This capability powers a wide range of business and consumer applications.",
    "keywords": [
      "written",
      "audio",
      "consumer",
      "input",
      "common",
      "powers",
      "capability",
      "speech",
      "speech-to-text",
      "also",
      "recognition",
      "range",
      "transcribes",
      "scenarios",
      "text",
      "called",
      "into",
      "wide",
      "listens",
      "applications"
    ]
  },
  {
    "id": 48,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Customer service and support",
    "content": "Service centers use speech recognition to:\n\n- Transcribe customer calls in real time for agent reference and quality assurance.\n- Route callers to the right department based on what they say.\n- Analyze call sentiment and identify common customer issues.\n- Generate searchable call records for compliance and training.\n\n**Business value**: Reduces manual note-taking, improves response accuracy, and captures insights that improve service quality.",
    "summary": "Service centers use speech recognition to:\n\n- Transcribe customer calls in real time for agent reference and quality assurance.\n- Route callers to the right department based on what they say.\n- Analyz...",
    "keywords": [
      "they",
      "real",
      "reference",
      "records",
      "insights",
      "transcribe",
      "support",
      "based",
      "assurance",
      "issues",
      "searchable",
      "accuracy",
      "generate",
      "improve",
      "common",
      "route",
      "callers",
      "time",
      "right",
      "customer"
    ]
  },
  {
    "id": 49,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Voice-activated assistants and agents",
    "content": "Virtual assistants and AI agents rely on speech recognition to:\n\n- Accept voice commands for hands-free control of devices and applications.\n- Answer questions using natural language understanding.\n- Complete tasks like setting reminders, sending messages, or searching information.\n- Control smart home devices, automotive systems, and wearable technology.\n\n**Business value**: Increases user engagement, simplifies complex workflows, and enables operation in situations where screens aren't practic",
    "summary": "Virtual assistants and AI agents rely on speech recognition to:\n\n- Accept voice commands for hands-free control of devices and applications.\n- Answer questions using natural language understanding.\n-...",
    "keywords": [
      "smart",
      "assistants",
      "voice-activated",
      "systems",
      "rely",
      "where",
      "simplifies",
      "complex",
      "practical",
      "language",
      "agents",
      "wearable",
      "automotive",
      "using",
      "aren",
      "virtual",
      "devices",
      "natural",
      "like",
      "voice"
    ]
  },
  {
    "id": 50,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Meeting and interview transcription",
    "content": "Organizations transcribe conversations to:\n\n- Create searchable meeting notes and action item lists.\n- Provide real-time captions for participants who are deaf or hard of hearing.\n- Generate summaries of interviews, focus groups, and research sessions.\n- Extract key discussion points for documentation and follow-up.\n\n**Business value**: Saves hours of manual transcription work, ensures accurate records, and makes spoken content accessible to everyone.",
    "summary": "Organizations transcribe conversations to:\n\n- Create searchable meeting notes and action item lists.\n- Provide real-time captions for participants who are deaf or hard of hearing.\n- Generate summaries...",
    "keywords": [
      "discussion",
      "captions",
      "records",
      "everyone",
      "research",
      "transcribe",
      "documentation",
      "item",
      "searchable",
      "generate",
      "summaries",
      "conversations",
      "notes",
      "sessions",
      "work",
      "provide",
      "groups",
      "manual",
      "organizations",
      "extract"
    ]
  },
  {
    "id": 51,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Healthcare documentation",
    "content": "Clinical professionals use speech recognition to:\n\n- Dictate patient notes directly into electronic health records.\n- Update treatment plans without interrupting patient care.\n- Reduce administrative burden and prevent physician burnout.\n- Improve documentation accuracy by capturing details in the moment.\n\n**Business value**: Increases time available for patient care, improves record completeness, and reduces documentation errors.",
    "summary": "Clinical professionals use speech recognition to:\n\n- Dictate patient notes directly into electronic health records.\n- Update treatment plans without interrupting patient care.\n- Reduce administrative...",
    "keywords": [
      "capturing",
      "records",
      "burnout",
      "documentation",
      "plans",
      "interrupting",
      "dictate",
      "completeness",
      "health",
      "care",
      "record",
      "healthcare",
      "accuracy",
      "improve",
      "notes",
      "without",
      "burden",
      "moment",
      "time",
      "physician"
    ]
  },
  {
    "id": 52,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Common speech synthesis scenarios",
    "content": "Speech synthesis, also called text-to-speech, converts written text into spoken audio. This technology creates voices for applications that need to communicate information audibly.",
    "summary": "Speech synthesis, also called text-to-speech, converts written text into spoken audio. This technology creates voices for applications that need to communicate information audibly.",
    "keywords": [
      "audio",
      "written",
      "creates",
      "audibly",
      "converts",
      "common",
      "technology",
      "need",
      "speech",
      "spoken",
      "also",
      "scenarios",
      "text",
      "called",
      "into",
      "synthesis",
      "information",
      "text-to-speech",
      "applications",
      "voices"
    ]
  },
  {
    "id": 53,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Conversational AI and chatbots",
    "content": "AI agents use speech synthesis to:\n\n- Respond to users with natural-sounding voices instead of requiring them to read text.\n- Create personalized interactions by adjusting tone, pace, and speaking style.\n- Handle customer inquiries through voice channels like phone systems.\n- Provide consistent brand experiences across voice and text interfaces.\n\n**Business value**: Makes AI agents more approachable, reduces customer effort, and extends service availability to voice-only channels.",
    "summary": "AI agents use speech synthesis to:\n\n- Respond to users with natural-sounding voices instead of requiring them to read text.\n- Create personalized interactions by adjusting tone, pace, and speaking sty...",
    "keywords": [
      "systems",
      "channels",
      "instead",
      "experiences",
      "inquiries",
      "interfaces",
      "adjusting",
      "tone",
      "more",
      "agents",
      "them",
      "interactions",
      "like",
      "voice",
      "personalized",
      "customer",
      "provide",
      "consistent",
      "service",
      "approachable"
    ]
  },
  {
    "id": 54,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Accessibility and content consumption",
    "content": "Applications generate audio to:\n\n- Read web content, articles, and documents aloud for users with visual impairments.\n- Support users with reading disabilities like dyslexia.\n- Enable content consumption while driving, exercising, or performing other tasks.\n- Provide audio alternatives for text-heavy interfaces.\n\n**Business value**: Expands your audience reach, demonstrates commitment to inclusion, and improves user satisfaction.",
    "summary": "Applications generate audio to:\n\n- Read web content, articles, and documents aloud for users with visual impairments.\n- Support users with reading disabilities like dyslexia.\n- Enable content consumpt...",
    "keywords": [
      "articles",
      "interfaces",
      "while",
      "support",
      "audience",
      "audio",
      "visual",
      "generate",
      "exercising",
      "enable",
      "like",
      "provide",
      "inclusion",
      "user",
      "dyslexia",
      "business",
      "content",
      "consumption",
      "documents",
      "satisfaction"
    ]
  },
  {
    "id": 55,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Notifications and alerts",
    "content": "Systems use speech synthesis to:\n\n- Announce important alerts, reminders, and status updates.\n- Provide navigation instructions in mapping and GPS applications.\n- Deliver time-sensitive information without requiring users to look at screens.\n- Communicate system status in industrial and operational environments.\n\n**Business value**: Ensures critical information reaches users even when visual attention isn't available, improving safety and responsiveness.",
    "summary": "Systems use speech synthesis to:\n\n- Announce important alerts, reminders, and status updates.\n- Provide navigation instructions in mapping and GPS applications.\n- Deliver time-sensitive information wi...",
    "keywords": [
      "systems",
      "even",
      "navigation",
      "mapping",
      "notifications",
      "visual",
      "critical",
      "time-sensitive",
      "without",
      "look",
      "deliver",
      "industrial",
      "system",
      "updates",
      "provide",
      "reaches",
      "status",
      "screens",
      "reminders",
      "attention"
    ]
  },
  {
    "id": 56,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "E-learning and training",
    "content": "Educational platforms use speech synthesis to:\n\n- Create narrated lessons and course content without recording studios.\n- Provide pronunciation examples for language learning.\n- Generate audio versions of written materials for different learning preferences.\n- Scale content production across multiple languages.\n\n**Business value**: Reduces content creation costs, supports diverse learning styles, and accelerates course development timelines.",
    "summary": "Educational platforms use speech synthesis to:\n\n- Create narrated lessons and course content without recording studios.\n- Provide pronunciation examples for language learning.\n- Generate audio version...",
    "keywords": [
      "development",
      "examples",
      "preferences",
      "course",
      "versions",
      "supports",
      "narrated",
      "audio",
      "language",
      "studios",
      "written",
      "generate",
      "e-learning",
      "timelines",
      "without",
      "recording",
      "creation",
      "educational",
      "provide",
      "scale"
    ]
  },
  {
    "id": 57,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Entertainment and media",
    "content": "Content creators use speech synthesis to:\n\n- Generate character voices for games and interactive experiences.\n- Produce podcast drafts and audiobook prototypes.\n- Create voiceovers for videos and presentations.\n- Personalize audio content based on user preferences.\n\n**Business value**: Lowers production costs, enables rapid prototyping, and creates customized experiences at scale.",
    "summary": "Content creators use speech synthesis to:\n\n- Generate character voices for games and interactive experiences.\n- Produce podcast drafts and audiobook prototypes.\n- Create voiceovers for videos and pres...",
    "keywords": [
      "experiences",
      "entertainment",
      "preferences",
      "prototyping",
      "personalize",
      "customized",
      "based",
      "audio",
      "rapid",
      "lowers",
      "creates",
      "generate",
      "interactive",
      "podcast",
      "produce",
      "videos",
      "voices",
      "games",
      "user",
      "scale"
    ]
  },
  {
    "id": 58,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Combining speech recognition and synthesis",
    "content": "The most powerful speech-enabled applications combine both capabilities to create conversational experiences:\n\n- **Voice-driven customer service**: Agents listen to customer questions (recognition), process the request, and respond with helpful answers (synthesis).\n- **Interactive voice response (IVR) systems**: Callers speak their needs, and the system guides them through options using natural dialogue.\n- **Language learning applications**: Students speak practice phrases (recognition), and the",
    "summary": "The most powerful speech-enabled applications combine both capabilities to create conversational experiences:\n\n- **Voice-driven customer service**: Agents listen to customer questions (recognition), p...",
    "keywords": [
      "actions",
      "systems",
      "powerful",
      "experiences",
      "request",
      "voice-controlled",
      "interfaces",
      "give",
      "language",
      "provides",
      "agents",
      "process",
      "them",
      "options",
      "interactive",
      "using",
      "practice",
      "phrases",
      "conversations",
      "capabilities"
    ]
  },
  {
    "id": 59,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Key considerations before implementing speech",
    "content": "Before you add speech capabilities to your application, evaluate these factors:\n\n- **Audio quality requirements**: Background noise, microphone quality, and network bandwidth affect speech recognition accuracy.\n- **Language and dialect support**: Verify that your target languages and regional variations are supported.\n- **Privacy and compliance**: Understand how audio data is processed, stored, and protected to meet regulatory requirements.\n- **Latency expectations**: Real-time conversations req",
    "summary": "Before you add speech capabilities to your application, evaluate these factors:\n\n- **Audio quality requirements**: Background noise, microphone quality, and network bandwidth affect speech recognition...",
    "keywords": [
      "even",
      "prefer",
      "implementation",
      "methods",
      "doesn",
      "privacy",
      "interfaces",
      "protected",
      "data",
      "support",
      "stored",
      "understand",
      "while",
      "audio",
      "language",
      "alternative",
      "delays",
      "accuracy",
      "input",
      "conversations"
    ]
  },
  {
    "id": 60,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Audio capture: Convert analog audio to digital",
    "content": "Speech recognition begins when a microphone converts sound waves into a digital signal. The system samples the analog audio thousands of times per second—typically 16,000 samples per second (16 kHz) for speech applications—and stores each measurement as a numeric value.\n\n![Diagram of an audio waveform.](../media/wave-form-spectogram.png)\n\n\n\nBefore moving to the next stage, the system often applies basic filters to remove hums, clicks, or other background noise that could confuse the model.",
    "summary": "Speech recognition begins when a microphone converts sound waves into a digital signal. The system samples the analog audio thousands of times per second—typically 16,000 samples per second (16 kHz) f...",
    "keywords": [
      "model",
      "times",
      "clicks",
      "diagram",
      "audio",
      "signal",
      "moving",
      "basic",
      "converts",
      "numeric",
      "applies",
      "noise",
      "wave-form-spectogram",
      "system",
      "microphone",
      "filters",
      "background",
      "stage",
      "hums",
      "begins"
    ]
  },
  {
    "id": 61,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Pre-processing: Extract meaningful features",
    "content": "Raw audio samples contain too much information for efficient pattern recognition. Pre-processing transforms the waveform into a compact representation that highlights speech characteristics while discarding irrelevant details like absolute volume.",
    "summary": "Raw audio samples contain too much information for efficient pattern recognition. Pre-processing transforms the waveform into a compact representation that highlights speech characteristics while disc...",
    "keywords": [
      "contain",
      "features",
      "characteristics",
      "irrelevant",
      "while",
      "pattern",
      "absolute",
      "audio",
      "compact",
      "representation",
      "like",
      "pre-processing",
      "volume",
      "much",
      "meaningful",
      "efficient",
      "discarding",
      "extract",
      "speech",
      "transforms"
    ]
  },
  {
    "id": 62,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Mel-Frequency Cepstral Coefficients (MFCCs)",
    "content": "MFCC is the most common feature extraction technique in speech recognition. It mimics how the human ear perceives sound by emphasizing frequencies where speech energy concentrates and compressing less important ranges.",
    "summary": "MFCC is the most common feature extraction technique in speech recognition. It mimics how the human ear perceives sound by emphasizing frequencies where speech energy concentrates and compressing less...",
    "keywords": [
      "concentrates",
      "technique",
      "emphasizing",
      "frequencies",
      "cepstral",
      "compressing",
      "human",
      "common",
      "ranges",
      "feature",
      "speech",
      "perceives",
      "coefficients",
      "recognition",
      "most",
      "mel-frequency",
      "less",
      "mfcc",
      "energy",
      "mfccs"
    ]
  },
  {
    "id": 63,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "How MFCC works:",
    "content": "1. **Divide audio into frames:** Split the signal into overlapping 20–30 millisecond windows.\n2. **Apply Fourier transform:** Convert each frame from time domain to frequency domain, revealing which pitches are present.\n3. **Map to Mel scale:** Adjust frequency bins to match human hearing sensitivity—we distinguish low pitches better than high ones.\n4. **Extract coefficients:** Compute a small set of numbers (often 13 coefficients) that summarize the spectral shape of each frame.\n\n![Diagram of a",
    "summary": "1. **Divide audio into frames:** Split the signal into overlapping 20–30 millisecond windows.\n2. **Apply Fourier transform:** Convert each frame from time domain to frequency domain, revealing which p...",
    "keywords": [
      "windows",
      "fourier",
      "spectral",
      "storing",
      "become",
      "diagram",
      "audio",
      "ones",
      "coefficient",
      "media",
      "pitches",
      "domain",
      "vector",
      "signal",
      "human",
      "revealing",
      "input",
      "without",
      "like",
      "values"
    ]
  },
  {
    "id": 64,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Acoustic modeling: Recognize phonemes",
    "content": "Acoustic models learn the relationship between audio features and **phonemes**—the smallest units of sound that distinguish words. English uses about 44 phonemes; for example, the word \"cat\" comprises three phonemes: /k/, /æ/, and /t/.",
    "summary": "Acoustic models learn the relationship between audio features and **phonemes**—the smallest units of sound that distinguish words. English uses about 44 phonemes; for example, the word \"cat\" comprises...",
    "keywords": [
      "three",
      "features",
      "models",
      "phonemes",
      "audio",
      "comprises",
      "english",
      "example",
      "between",
      "units",
      "acoustic",
      "modeling",
      "learn",
      "smallest",
      "words",
      "about",
      "word",
      "uses",
      "sound",
      "distinguish"
    ]
  },
  {
    "id": 65,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "From features to phonemes",
    "content": "Modern acoustic models use **transformer architectures**, a type of deep learning network that excels at sequence tasks. The transformer processes the MFCC feature vectors and predicts which phoneme is most likely at each moment in time.\n\nTransformer models achieve effective phoneme prediction through: \n\n- **Attention mechanism:** The model examines surrounding frames to resolve ambiguity. For example, the phoneme /t/ sounds different at the start of \"top\" versus the end of \"bat.\"\n- **Parallel p",
    "summary": "Modern acoustic models use **transformer architectures**, a type of deep learning network that excels at sequence tasks. The transformer processes the MFCC feature vectors and predicts which phoneme i...",
    "keywords": [
      "unlike",
      "simultaneously",
      "excels",
      "model",
      "features",
      "models",
      "resolve",
      "over",
      "deep",
      "versus",
      "phonemes",
      "audio",
      "sequences",
      "accuracy",
      "start",
      "contextualized",
      "moment",
      "example",
      "time",
      "frequently"
    ]
  },
  {
    "id": 66,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Language modeling: Predict word sequences",
    "content": "Phoneme predictions alone don't guarantee accurate transcription. The acoustic model might confuse \"their\" and \"there\" because they share identical phonemes. Language models resolve ambiguity by applying knowledge of vocabulary, grammar, and common word patterns. Some ways in which the model guides word sequence prediction include:\n\n- **Statistical patterns:** The model knows \"The weather is nice\" appears more often in training data than \"The whether is nice.\"\n- **Context awareness:** After hear",
    "summary": "Phoneme predictions alone don't guarantee accurate transcription. The acoustic model might confuse \"their\" and \"there\" because they share identical phonemes. Language models resolve ambiguity by apply...",
    "keywords": [
      "applying",
      "they",
      "knowledge",
      "model",
      "after",
      "models",
      "resolve",
      "data",
      "phonemes",
      "language",
      "weather",
      "more",
      "sequences",
      "accuracy",
      "domain",
      "alone",
      "legal",
      "custom",
      "table",
      "improve"
    ]
  },
  {
    "id": 67,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Decoding: Select the best text hypothesis",
    "content": "Decoding algorithms search through millions of possible word sequences to find the transcription that best matches both acoustic and language model predictions. This stage balances two competing goals: staying faithful to the audio signal while producing readable, grammatically correct text.",
    "summary": "Decoding algorithms search through millions of possible word sequences to find the transcription that best matches both acoustic and language model predictions. This stage balances two competing goals...",
    "keywords": [
      "best",
      "model",
      "correct",
      "readable",
      "find",
      "while",
      "audio",
      "language",
      "balances",
      "decoding",
      "algorithms",
      "sequences",
      "signal",
      "goals",
      "staying",
      "stage",
      "acoustic",
      "grammatically",
      "hypothesis",
      "matches"
    ]
  },
  {
    "id": 68,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Beam search decoding:",
    "content": "The most common technique, *beam search*, maintains a shortlist (the \"beam\") of top-scoring partial transcriptions as it processes each audio frame. At every step, it extends each hypothesis with the next most likely word, prunes low-scoring paths, and keeps only the best candidates.\n\nFor a three-second utterance, the decoder might evaluate thousands of hypotheses before selecting \"Please send the report by Friday\" over alternatives like \"Please sent the report buy Friday.\"\n\n> [!CAUTION]\n> Decod",
    "summary": "The most common technique, *beam search*, maintains a shortlist (the \"beam\") of top-scoring partial transcriptions as it processes each audio frame. At every step, it extends each hypothesis with the...",
    "keywords": [
      "best",
      "technique",
      "over",
      "step",
      "candidates",
      "prunes",
      "audio",
      "decoding",
      "caution",
      "accuracy",
      "send",
      "paths",
      "keeps",
      "limiting",
      "common",
      "like",
      "depth",
      "beam",
      "processes",
      "shortlist"
    ]
  },
  {
    "id": 69,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Post-processing: Refine the output",
    "content": "The decoder produces raw text that often requires cleanup before presentation. Post-processing applies formatting rules and corrections to improve readability and accuracy.",
    "summary": "The decoder produces raw text that often requires cleanup before presentation. Post-processing applies formatting rules and corrections to improve readability and accuracy.",
    "keywords": [
      "formatting",
      "post-processing",
      "text",
      "often",
      "decoder",
      "rules",
      "improve",
      "refine",
      "applies",
      "presentation",
      "readability",
      "output",
      "corrections",
      "before",
      "cleanup",
      "requires",
      "produces",
      "accuracy"
    ]
  },
  {
    "id": 70,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Common post-processing tasks:",
    "content": "- **Capitalization:** Convert \"hello my name is sam\" to \"Hello my name is Sam.\"\n- **Punctuation restoration:** Add periods, commas, and question marks based on prosody and grammar.\n- **Number formatting:** Change \"one thousand twenty three\" to \"1,023.\"\n- **Profanity filtering:** Mask or remove inappropriate words when required by policy.\n- **Inverse text normalization:** Convert spoken forms like \"three p m\" to \"3 PM.\"\n- **Confidence scoring:** Flag low-confidence words for human review in criti",
    "summary": "- **Capitalization:** Convert \"hello my name is sam\" to \"Hello my name is Sam.\"\n- **Punctuation restoration:** Add periods, commas, and question marks based on prosody and grammar.\n- **Number formatti...",
    "keywords": [
      "fallback",
      "hello",
      "three",
      "inappropriate",
      "forms",
      "segments",
      "prosody",
      "name",
      "restoration",
      "based",
      "scores",
      "critical",
      "formatting",
      "low-confidence",
      "human",
      "behaviors",
      "required",
      "highlight",
      "common",
      "like"
    ]
  },
  {
    "id": 71,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "How the pipeline works together",
    "content": "Each stage builds on the previous one:\n\n1. **Audio capture** provides the raw signal.\n2. **Pre-processing** extracts MFCC features that highlight speech patterns.\n3. **Acoustic modeling** predicts phoneme probabilities using transformer networks.\n4. **Language modeling** applies vocabulary and grammar knowledge.\n5. **Decoding** searches for the best word sequence.\n6. **Post-processing** formats the text for human readers.\n\nBy separating concerns, modern speech recognition systems achieve high ac",
    "summary": "Each stage builds on the previous one:\n\n1. **Audio capture** provides the raw signal.\n2. **Pre-processing** extracts MFCC features that highlight speech patterns.\n3. **Acoustic modeling** predicts pho...",
    "keywords": [
      "systems",
      "knowledge",
      "best",
      "model",
      "features",
      "probabilities",
      "insufficient",
      "audio",
      "language",
      "provides",
      "decoding",
      "poor",
      "accuracy",
      "signal",
      "using",
      "human",
      "highlight",
      "applies",
      "aggressive",
      "short"
    ]
  },
  {
    "id": 72,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Text normalization: Standardize the text",
    "content": "Text normalization prepares raw text for pronunciation by expanding abbreviations, numbers, and symbols into spoken forms.\n\nConsider the sentence: \"*Dr. Smith ordered 3 items for $25.50 on 12/15/2023.*\"\n\nA normalization system converts it to: \"Doctor Smith ordered three items for twenty-five dollars and fifty cents on December fifteenth, two thousand twenty-three.\"\n\nCommon normalization tasks include:\n\n- Expanding abbreviations (\"Dr.\" becomes \"Doctor\", \"Inc.\" becomes \"Incorporated\")\n- Converting",
    "summary": "Text normalization prepares raw text for pronunciation by expanding abbreviations, numbers, and symbols into spoken forms.\n\nConsider the sentence: \"*Dr. Smith ordered 3 items for $25.50 on 12/15/2023....",
    "keywords": [
      "homographs",
      "ordered",
      "zero",
      "past",
      "2023",
      "three",
      "times",
      "forms",
      "converting",
      "versus",
      "cents",
      "based",
      "special",
      "point",
      "converts",
      "standardize",
      "abbreviations",
      "doctor",
      "common",
      "attempting"
    ]
  },
  {
    "id": 73,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Linguistic analysis: Map text to phonemes",
    "content": "Linguistic analysis breaks normalized text into *phonemes* (the smallest units of sound) and determines how to pronounce each word. The linguistic analysis stage:\n\n1. Segments text into words and syllables.\n2. Looks up word pronunciations in lexicons (pronunciation dictionaries).\n3. Applies G2P rules or neural models to handle unknown words.\n4. Marks syllable boundaries and identifies stressed syllables.\n5. Determines phonetic context for adjacent sounds.",
    "summary": "Linguistic analysis breaks normalized text into *phonemes* (the smallest units of sound) and determines how to pronounce each word. The linguistic analysis stage:\n\n1. Segments text into words and syll...",
    "keywords": [
      "segments",
      "models",
      "analysis",
      "normalized",
      "identifies",
      "boundaries",
      "phonemes",
      "syllables",
      "unknown",
      "applies",
      "phonetic",
      "adjacent",
      "linguistic",
      "units",
      "marks",
      "stage",
      "breaks",
      "syllable",
      "pronounce",
      "smallest"
    ]
  },
  {
    "id": 74,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Grapheme-to-phoneme conversion",
    "content": "Grapheme-to-phoneme (G2P) conversion maps written letters (*graphemes*) to pronunciation sounds (*phonemes*). English spelling doesn't reliably indicate pronunciation, so G2P systems use both rules and learned patterns.\n\nFor example:\n\n- The word \"though\" converts to /θoʊ/\n- The word \"through\" converts to /θruː/\n- The word \"cough\" converts to /kɔːf/\n\nEach word contains the letters \"ough\", but the pronunciation differs dramatically.\n\nModern G2P systems use neural networks trained on pronunciation ",
    "summary": "Grapheme-to-phoneme (G2P) conversion maps written letters (*graphemes*) to pronunciation sounds (*phonemes*). English spelling doesn't reliably indicate pronunciation, so G2P systems use both rules an...",
    "keywords": [
      "systems",
      "grapheme-to-phoneme",
      "ough",
      "past",
      "model",
      "doesn",
      "dramatically",
      "models",
      "proper",
      "analysis",
      "pronounced",
      "phonemes",
      "versus",
      "written",
      "more",
      "books",
      "converts",
      "cough",
      "when",
      "differently"
    ]
  },
  {
    "id": 75,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Prosody generation: Determine pronunciation",
    "content": "Prosody refers to the rhythm, stress, and intonation patterns that make speech sound natural. Prosody generation determines **how** to say words, not just **which sounds** to produce.",
    "summary": "Prosody refers to the rhythm, stress, and intonation patterns that make speech sound natural. Prosody generation determines **how** to say words, not just **which sounds** to produce.",
    "keywords": [
      "just",
      "prosody",
      "refers",
      "rhythm",
      "produce",
      "natural",
      "speech",
      "generation",
      "stress",
      "which",
      "determines",
      "pronunciation",
      "words",
      "sounds",
      "intonation",
      "make",
      "patterns",
      "determine",
      "sound"
    ]
  },
  {
    "id": 76,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Elements of prosody",
    "content": "Prosody encompasses several vocal characteristics:\n\n- **Pitch contours**: Rising or falling pitch patterns that signal questions versus statements\n- **Duration**: How long to hold each sound, creating emphasis or natural rhythm\n- **Intensity**: Volume variations that highlight important words\n- **Pauses**: Breaks between phrases or sentences that aid comprehension\n- **Stress patterns**: Which syllables receive emphasis within words and sentences\n\nProsody has a significant effect on how spoken te",
    "summary": "Prosody encompasses several vocal characteristics:\n\n- **Pitch contours**: Rising or falling pitch patterns that signal questions versus statements\n- **Duration**: How long to hold each sound, creating...",
    "keywords": [
      "changes",
      "pauses",
      "statements",
      "cake",
      "characteristics",
      "encompasses",
      "prosody",
      "pitch",
      "contours",
      "duration",
      "versus",
      "meaning",
      "comprehension",
      "never",
      "emphasis",
      "rhythm",
      "vocal",
      "syllables",
      "several",
      "signal"
    ]
  },
  {
    "id": 77,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Transformer-based prosody prediction",
    "content": "Modern speech synthesis systems use transformer neural networks to predict prosody. Transformers excel at understanding context across entire sentences, not just adjacent words.",
    "summary": "Modern speech synthesis systems use transformer neural networks to predict prosody. Transformers excel at understanding context across entire sentences, not just adjacent words.",
    "keywords": [
      "systems",
      "entire",
      "just",
      "prosody",
      "predict",
      "understanding",
      "adjacent",
      "modern",
      "speech",
      "sentences",
      "transformer-based",
      "across",
      "words",
      "transformers",
      "context",
      "synthesis",
      "networks",
      "prediction",
      "neural",
      "excel"
    ]
  },
  {
    "id": 78,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The prosody generation process:",
    "content": "1. **Input encoding**: The transformer receives the phoneme sequence with linguistic features (punctuation, part of speech, sentence structure)\n2. **Contextual analysis**: Self-attention mechanisms identify relationships between words (for example, which noun a pronoun references, where sentence boundaries fall)\n3. **Prosody prediction**: The model outputs predicted values for pitch, duration, and energy at each phoneme\n4. **Style factors**: The system considers speaking style (neutral, expressi",
    "summary": "1. **Input encoding**: The transformer receives the phoneme sequence with linguistic features (punctuation, part of speech, sentence structure)\n2. **Contextual analysis**: Self-attention mechanisms id...",
    "keywords": [
      "contextual",
      "fall",
      "pauses",
      "model",
      "features",
      "pitch",
      "characteristics",
      "prosody",
      "analysis",
      "duration",
      "boundaries",
      "rise",
      "self-attention",
      "structure",
      "process",
      "pronoun",
      "input",
      "signal",
      "recorded",
      "neutral"
    ]
  },
  {
    "id": 79,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Factors influencing prosody choices:",
    "content": "- **Syntax**: Clause boundaries indicate where to pause\n- **Semantics**: Important concepts receive emphasis\n- **Discourse context**: Contrasting information or answers to questions may carry extra stress\n- **Speaker identity**: Each voice has characteristic pitch range and speaking rate\n- **Emotional tone**: Excitement, concern, or neutrality shape prosodic patterns\n\nThe prosody predictions create a target specification: \"Produce the phoneme /æ/ at 180 Hz for 80 milliseconds with moderate inten",
    "summary": "- **Syntax**: Clause boundaries indicate where to pause\n- **Semantics**: Important concepts receive emphasis\n- **Discourse context**: Contrasting information or answers to questions may carry extra st...",
    "keywords": [
      "results",
      "clause",
      "dramatically",
      "pitch",
      "emotional",
      "prosody",
      "identity",
      "milliseconds",
      "boundaries",
      "choices",
      "carry",
      "tone",
      "emphasis",
      "produce",
      "prosodic",
      "voice",
      "indicate",
      "syntax",
      "speaker",
      "rate"
    ]
  },
  {
    "id": 80,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Speech synthesis: Generate audio",
    "content": "Speech synthesis generates the final audio waveform based on the phoneme sequence and prosody specifications.",
    "summary": "Speech synthesis generates the final audio waveform based on the phoneme sequence and prosody specifications.",
    "keywords": [
      "generate",
      "speech",
      "generates",
      "synthesis",
      "sequence",
      "final",
      "prosody",
      "specifications",
      "phoneme",
      "based",
      "audio",
      "waveform"
    ]
  },
  {
    "id": 81,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Waveform generation approaches",
    "content": "Modern systems use neural vocoders—deep learning models that generate audio samples directly. Popular vocoder architectures include WaveNet, WaveGlow, and HiFi-GAN.",
    "summary": "Modern systems use neural vocoders—deep learning models that generate audio samples directly. Popular vocoder architectures include WaveNet, WaveGlow, and HiFi-GAN.",
    "keywords": [
      "systems",
      "models",
      "deep",
      "audio",
      "generate",
      "popular",
      "waveglow",
      "vocoder",
      "modern",
      "include",
      "directly",
      "generation",
      "samples",
      "waveform",
      "learning",
      "hifi-gan",
      "wavenet",
      "approaches",
      "neural",
      "architectures"
    ]
  },
  {
    "id": 82,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The synthesis process:",
    "content": "1. **Acoustic feature generation**: An acoustic model (often a transformer) converts phonemes and prosody targets into mel-spectrograms—visual representations of sound frequencies over time\n2. **Vocoding**: The neural vocoder converts mel-spectrograms into raw audio waveforms (sequences of amplitude values at 16,000-48,000 samples per second)\n3. **Post-processing**: The system applies filtering, normalization, or audio effects to match target output specifications\n\n\n\nThe vocoder essentially perf",
    "summary": "1. **Acoustic feature generation**: An acoustic model (often a transformer) converts phonemes and prosody targets into mel-spectrograms—visual representations of sound frequencies over time\n2. **Vocod...",
    "keywords": [
      "model",
      "prosody",
      "over",
      "frequencies",
      "while",
      "phonemes",
      "targets",
      "performs",
      "audio",
      "visual",
      "sequences",
      "process",
      "vocoding",
      "converts",
      "applies",
      "values",
      "time",
      "output",
      "specifications",
      "vocoder"
    ]
  },
  {
    "id": 83,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The complete pipeline in action",
    "content": "When you request speech synthesis for \"Dr. Chen's appointment is at 3:00 PM\":\n\n1. **Text normalization** expands it to \"Doctor Chen's appointment is at three o'clock P M\"\n2. **Linguistic analysis** converts it to phonemes: /ˈdɑktər ˈtʃɛnz əˈpɔɪntmənt ɪz æt θri əˈklɑk pi ɛm/\n3. **Prosody generation** predicts pitch rising slightly on \"appointment\", a pause after \"is\", and emphasis on \"three\"\n4. **Speech synthesis** generates an audio waveform matching those specifications\n\nThe entire process typi",
    "summary": "When you request speech synthesis for \"Dr. Chen's appointment is at 3:00 PM\":\n\n1. **Text normalization** expands it to \"Doctor Chen's appointment is at three o'clock P M\"\n2. **Linguistic analysis** co...",
    "keywords": [
      "three",
      "appointment",
      "request",
      "matching",
      "entire",
      "after",
      "pitch",
      "prosody",
      "analysis",
      "phonemes",
      "audio",
      "emphasis",
      "process",
      "converts",
      "doctor",
      "chen",
      "linguistic",
      "specifications",
      "modern",
      "normalization"
    ]
  }
]