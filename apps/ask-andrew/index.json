[
  {
    "id": 1,
    "category": "Generative AI",
    "file": "01-language-models.md",
    "heading": "Tokenization",
    "content": "The first step is to provide the model with a large vocabulary of words and phrases; and we do mean *large*. The latest generation of LLMs have vocabularies that consist of hundreds of thousands of tokens, based on large volumes of training data from across the Internet and other sources.\n\nWait a minute. *Tokens*?\n\nWhile we tend to think of language in terms of *words*, LLMs break down their vocabulary into *tokens*. Tokens include words, but also *sub*-words (like the \"un\" in \"unbelievable\" and",
    "summary": "The first step is to provide the model with a large vocabulary of words and phrases; and we do mean *large*. The latest generation of LLMs have vocabularies that consist of hundreds of thousands of to...",
    "keywords": [
      "tokens",
      "words",
      "training",
      "vocabulary",
      "step",
      "model",
      "llms",
      "data",
      "language",
      "break",
      "assigned",
      "tokenization",
      "gpt",
      "assistant",
      "embedding",
      "prompt",
      "word",
      "copilot",
      "chat",
      "chatbot",
      "completion",
      "large-language-model",
      "llm",
      "foundation-model"
    ]
  },
  {
    "id": 2,
    "category": "Generative AI",
    "file": "01-language-models.md",
    "heading": "Transforming tokens with a *transformer*",
    "content": "Now that we have a set of tokens with unique IDs, we need to find a way to relate them to one another. To do this, we assign each token a *vector* (an array of multiple numeric values, like [1, 23, 45]). Each vector has multiple numeric *elements* or *dimensions*, and we can use these to encode linguistic and semantic attributes of the token to help provide a great deal of information about what the token *means* and how it relates to other tokens, in an efficient format.\n\nWe need to transform t",
    "summary": "Now that we have a set of tokens with unique IDs, we need to find a way to relate them to one another. To do this, we assign each token a *vector* (an array of multiple numeric values, like [1, 23, 45...",
    "keywords": [
      "token",
      "tokens",
      "vector",
      "attention",
      "transformer",
      "multiple",
      "values",
      "semantic",
      "embeddings",
      "layer",
      "find",
      "assign",
      "gpt",
      "vocabulary",
      "extraction",
      "chatbot",
      "request",
      "instruction",
      "deep-learning",
      "query",
      "assistant",
      "neurons",
      "field",
      "completion",
      "layers",
      "backpropagation",
      "llm",
      "template",
      "input",
      "word",
      "copilot",
      "chat",
      "tokenization",
      "prompt",
      "structured-data",
      "embedding"
    ]
  },
  {
    "id": 3,
    "category": "Generative AI",
    "file": "01-language-models.md",
    "heading": "Initial vectors and positional encoding",
    "content": "Initially, the token vector values are assigned randomly, before being fed through the transformer to create embedding vectors. The token vectors are fed into the transformer along with a *positional encoding* that indicates where the token appears in the sequence of training text (we need to do this because the order in which tokens appear in the sequence is relevant to how they relate to one another). For example, our tokens might start off looking like this:\n\n|Token | Token ID | Position | Ve",
    "summary": "Initially, the token vector values are assigned randomly, before being fed through the transformer to create embedding vectors. The token vectors are fed into the transformer along with a *positional...",
    "keywords": [
      "token",
      "vectors",
      "positional",
      "encoding",
      "vector",
      "transformer",
      "sequence",
      "tokens",
      "initial",
      "initially",
      "values",
      "assigned",
      "gpt",
      "assistant",
      "prompt",
      "word",
      "copilot",
      "chat",
      "chatbot",
      "vocabulary",
      "completion",
      "tokenization",
      "field",
      "extraction",
      "structured-data",
      "llm",
      "embedding",
      "template"
    ]
  },
  {
    "id": 4,
    "category": "Generative AI",
    "file": "01-language-models.md",
    "heading": "Attention and embeddings",
    "content": "To determine the vector representations of tokens that include embedded contextual information, the transformer uses *attention* layers. An attention layer considers each token in turn, within the context of the sequence of tokens in which it appears. The tokens around the current one are weighted to reflect their influence and the weights are used to calculate the element values for the current token's embedding vector. For example, when considering the token \"bark\" in the context of \"I heard a",
    "summary": "To determine the vector representations of tokens that include embedded contextual information, the transformer uses *attention* layers. An attention layer considers each token in turn, within the con...",
    "keywords": [
      "tokens",
      "embeddings",
      "bark",
      "vectors",
      "token",
      "embedding",
      "example",
      "elements",
      "attention",
      "different",
      "vector",
      "context",
      "gpt",
      "assistant",
      "prompt",
      "word",
      "copilot",
      "chat",
      "chatbot",
      "vocabulary",
      "completion",
      "tokenization",
      "field",
      "extraction",
      "structured-data",
      "llm",
      "template"
    ]
  },
  {
    "id": 5,
    "category": "Generative AI",
    "file": "01-language-models.md",
    "heading": "Predicting completions from prompts",
    "content": "Now that we have a set of embeddings that encapsulate the contextual relationship between tokens, we can use the *decoder* block of a transformer to iteratively predict the next word in a sequence based on a starting *prompt*.\n\nOnce again, *attention* is used to consider each token in context; but this time the context to be considered can only include the tokens that *precede* the token we're trying to predict. The decoder model is trained, using data for which we already have the full sequence",
    "summary": "Now that we have a set of embeddings that encapsulate the contextual relationship between tokens, we can use the *decoder* block of a transformer to iteratively predict the next word in a sequence bas...",
    "keywords": [
      "token",
      "sequence",
      "tokens",
      "predict",
      "attention",
      "decoder",
      "model",
      "predicting",
      "transformer",
      "context",
      "already",
      "training",
      "gpt",
      "vocabulary",
      "extraction",
      "chatbot",
      "request",
      "instruction",
      "query",
      "assistant",
      "field",
      "completion",
      "llm",
      "template",
      "input",
      "word",
      "copilot",
      "chat",
      "tokenization",
      "prompt",
      "structured-data",
      "embedding"
    ]
  },
  {
    "id": 6,
    "category": "Generative AI",
    "file": "02-writing-prompts.md",
    "heading": "Types of prompt",
    "content": "There are two main types of prompts:\n\n- **System prompts** that set the behavior and tone of the model, and any constraints it should adhere to. For example, \"*You're a helpful assistant that responds in a cheerful, friendly manner.*\". System prompts determine constraints and styles for the model's responses.\n- **User prompts** that elicit a response to a specific question or instruction. For example, \"*Summarize the key considerations for adopting generative AI described in <u>GenAI_Considerati",
    "summary": "There are two main types of prompts:\n\n- **System prompts** that set the behavior and tone of the model, and any constraints it should adhere to. For example, \"*You're a helpful assistant that responds...",
    "keywords": [
      "prompts",
      "user",
      "system",
      "prompt",
      "model",
      "application",
      "types",
      "tone",
      "constraints",
      "example",
      "responds",
      "considerations",
      "gpt",
      "assistant",
      "artificial-intelligence",
      "template",
      "input",
      "copilot",
      "chat",
      "chatbot",
      "field",
      "completion",
      "extraction",
      "request",
      "ai",
      "structured-data",
      "llm",
      "instruction",
      "query"
    ]
  },
  {
    "id": 7,
    "category": "Generative AI",
    "file": "02-writing-prompts.md",
    "heading": "Conversation history",
    "content": "To keep a conversation consistent and relevant, generative AI apps often keep track of the conversation history; and include summarized versions of it in subsequent prompts. This ensures there’s an ongoing context for the conversation that the model can build on.\n\n![Diagram of a conversation with multiple prompts and completions.](../media/conversation-history.png)\n\nFor example, suppose the model responds to the system and user prompts described previously with the following completion:\n\n> *Key ",
    "summary": "To keep a conversation consistent and relevant, generative AI apps often keep track of the conversation history; and include summarized versions of it in subsequent prompts. This ensures there’s an on...",
    "keywords": [
      "conversation",
      "prompts",
      "generative",
      "model",
      "question",
      "history",
      "keep",
      "context",
      "build",
      "risks",
      "consistent",
      "relevant",
      "gpt",
      "assistant",
      "artificial-intelligence",
      "prompt",
      "input",
      "copilot",
      "chat",
      "chatbot",
      "completion",
      "request",
      "ai",
      "llm",
      "instruction",
      "query"
    ]
  },
  {
    "id": 8,
    "category": "Generative AI",
    "file": "02-writing-prompts.md",
    "heading": "Retrieval augmented generation (RAG)",
    "content": "To add even more context, generative AI applications can use a technique called *retrieval augmented generation (RAG)*. This approach involves retrieving information, like documents or emails, and using it to augment the prompt with relevant data. The response generated by the model is then *grounded* in the information that was provided.\n\nFor example, suppose you submit a prompt like \"*What's the maximum I can claim for travel expenses on a business trip?*\". With no other information, a model w",
    "summary": "To add even more context, generative AI applications can use a technique called *retrieval augmented generation (RAG)*. This approach involves retrieving information, like documents or emails, and usi...",
    "keywords": [
      "expenses",
      "information",
      "prompt",
      "model",
      "retrieval",
      "augmented",
      "generation",
      "context",
      "policy",
      "retrieving",
      "relevant",
      "travel",
      "gpt",
      "search",
      "extraction",
      "chatbot",
      "request",
      "instruction",
      "query",
      "assistant",
      "artificial-intelligence",
      "field",
      "completion",
      "llm",
      "template",
      "input",
      "copilot",
      "chat",
      "grounding",
      "ai",
      "structured-data"
    ]
  },
  {
    "id": 9,
    "category": "Generative AI",
    "file": "02-writing-prompts.md",
    "heading": "Tips for better prompts",
    "content": "The quality of responses from generative AI assistants not only depends on the language model used, but on the prompts you submit to it.\n\n![Diagram of a clear and specific prompt with context, examples, and a request for structure.](../media/writing-prompts.png)\n\nTo get better results from your prompts:\n\n- Be **clear** and **specific** – prompts with explicit instructions or questions work better than vague language.\n- Add **context** - mention the topic, audience, or format you want.\n- Use **ex",
    "summary": "The quality of responses from generative AI assistants not only depends on the language model used, but on the prompts you submit to it.\n\n![Diagram of a clear and specific prompt with context, example...",
    "keywords": [
      "prompts",
      "better",
      "generative",
      "language",
      "model",
      "clear",
      "specific",
      "context",
      "examples",
      "structure",
      "results",
      "tips",
      "gpt",
      "assistant",
      "artificial-intelligence",
      "template",
      "prompt",
      "input",
      "copilot",
      "chat",
      "chatbot",
      "field",
      "completion",
      "extraction",
      "request",
      "ai",
      "structured-data",
      "llm",
      "instruction",
      "query"
    ]
  },
  {
    "id": 10,
    "category": "Generative AI",
    "file": "03-agents.md",
    "heading": "Components of an AI agent",
    "content": "![Diagram of an agent with a model, instructions, and tools.](../media/agent.png)\n\nAI agents have three key elements:\n\n- **A large language model**: This is the agent's brain; using generative AI for language understanding and reasoning.\n- **Instructions**: A system prompt that defines the agent’s role and behavior. Think of it as the agent’s job description.\n- **Tools**: These are what the agent uses to interact with the world. Tools can include:\n    - *Knowledge* tools that provide access to i",
    "summary": "![Diagram of an agent with a model, instructions, and tools.](../media/agent.png)\n\nAI agents have three key elements:\n\n- **A large language model**: This is the agent's brain; using generative AI for...",
    "keywords": [
      "agent",
      "tools",
      "model",
      "instructions",
      "agents",
      "language",
      "role",
      "tasks",
      "components",
      "diagram",
      "media",
      "three",
      "gpt",
      "extraction",
      "chatbot",
      "request",
      "instruction",
      "query",
      "assistant",
      "artificial-intelligence",
      "field",
      "completion",
      "function-calling",
      "llm",
      "tool-use",
      "template",
      "input",
      "copilot",
      "chat",
      "large-language-model",
      "foundation-model",
      "planning",
      "prompt",
      "autonomous",
      "ai",
      "structured-data",
      "llms"
    ]
  },
  {
    "id": 11,
    "category": "Generative AI",
    "file": "03-agents.md",
    "heading": "Multi-agent systems",
    "content": "Agents can also work with one another, in multi-agent systems. Instead of one agent doing everything, multiple agents can collaborate—each with its own specialty. One might gather data, another might analyze it, and a third might take action. Together, they form an AI-powered workforce that can handle complex workflows, just like a human team.\n\n![Diagram of a multi-agent system.](../media/multiple-agents.png)\n\nAgents communicate with each other through prompts, using generative AI to determine w",
    "summary": "Agents can also work with one another, in multi-agent systems. Instead of one agent doing everything, multiple agents can collaborate—each with its own specialty. One might gather data, another might...",
    "keywords": [
      "agents",
      "multi-agent",
      "systems",
      "another",
      "instead",
      "agent",
      "doing",
      "everything",
      "multiple",
      "collaborate",
      "specialty",
      "gather",
      "gpt",
      "extraction",
      "chatbot",
      "request",
      "instruction",
      "query",
      "assistant",
      "artificial-intelligence",
      "field",
      "completion",
      "function-calling",
      "llm",
      "tool-use",
      "template",
      "input",
      "copilot",
      "chat",
      "planning",
      "prompt",
      "autonomous",
      "ai",
      "structured-data"
    ]
  },
  {
    "id": 12,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Supervised machine learning",
    "content": "*Supervised* machine learning is a general term for machine learning algorithms in which the training data includes both *feature* values and known *label* values. Supervised machine learning is used to train models by determining a relationship between the features and labels in past observations, so that unknown labels can be predicted for features in future cases.",
    "summary": "*Supervised* machine learning is a general term for machine learning algorithms in which the training data includes both *feature* values and known *label* values. Supervised machine learning is used...",
    "keywords": [
      "machine",
      "learning",
      "supervised",
      "values",
      "features",
      "labels",
      "general",
      "term",
      "algorithms",
      "training",
      "data",
      "feature",
      "analytics",
      "algorithm",
      "prediction",
      "model",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 13,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Regression",
    "content": "*Regression* is a form of supervised machine learning in which the label predicted by the model is a numeric value. For example:\n\n- The number of ice creams sold on a given day, based on the temperature, rainfall, and windspeed.\n- The selling price of a property based on its size in square feet, the number of bedrooms it contains, and socio-economic metrics for its location.\n- The fuel efficiency (in miles-per-gallon) of a car based on its engine size, weight, width, height, and length.",
    "summary": "*Regression* is a form of supervised machine learning in which the label predicted by the model is a numeric value. For example:\n\n- The number of ice creams sold on a given day, based on the temperatu...",
    "keywords": [
      "based",
      "regression",
      "number",
      "size",
      "form",
      "supervised",
      "machine",
      "learning",
      "label",
      "predicted",
      "model",
      "numeric",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "continuous",
      "forecasting",
      "extraction",
      "training",
      "structured-data",
      "estimation",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 14,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Classification",
    "content": "*Classification* is a form of supervised machine learning in which the label represents a categorization, or *class*. There are two common classification scenarios.",
    "summary": "*Classification* is a form of supervised machine learning in which the label represents a categorization, or *class*. There are two common classification scenarios.",
    "keywords": [
      "classification",
      "form",
      "supervised",
      "machine",
      "learning",
      "label",
      "represents",
      "categorization",
      "class",
      "common",
      "scenarios",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "model",
      "extraction",
      "sorting",
      "training",
      "labeling",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 15,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Binary classification",
    "content": "In *binary classification*, the label determines whether the observed item *is* (or *isn't*) an instance of a specific class. Or put another way, binary classification models predict one of two mutually exclusive outcomes. For example:\n\n- Whether a patient is at risk for diabetes based on clinical metrics like weight, age, blood glucose level, and so on.\n- Whether a bank customer will default on a loan based on income, credit history, age, and other factors.\n- Whether a mailing list customer wil",
    "summary": "In *binary classification*, the label determines whether the observed item *is* (or *isn't*) an instance of a specific class. Or put another way, binary classification models predict one of two mutual...",
    "keywords": [
      "binary",
      "classification",
      "based",
      "class",
      "customer",
      "label",
      "determines",
      "observed",
      "item",
      "instance",
      "specific",
      "another",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "model",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 16,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Multiclass classification",
    "content": "*Multiclass classification* extends binary classification to predict a label that represents one of multiple possible classes. For example,\n\n- The species of a penguin (*Adelie*, *Gentoo*, or *Chinstrap*) based on its physical measurements.\n- The genre of a movie (*comedy*, *horror*, *romance*, *adventure*, or *science fiction*) based on its cast, director, and budget.\n\nIn most scenarios that involve a known set of multiple classes, multiclass classification is used to predict mutually exclusive",
    "summary": "*Multiclass classification* extends binary classification to predict a label that represents one of multiple possible classes. For example,\n\n- The species of a penguin (*Adelie*, *Gentoo*, or *Chinstr...",
    "keywords": [
      "classification",
      "multiclass",
      "example",
      "predict",
      "label",
      "multiple",
      "classes",
      "penguin",
      "adelie",
      "gentoo",
      "based",
      "movie",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "model",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 17,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Unsupervised machine learning",
    "content": "*Unsupervised* machine learning involves training models using data that consists only of *feature* values without any known labels. Unsupervised machine learning algorithms determine relationships between the features of the observations in the training data.",
    "summary": "*Unsupervised* machine learning involves training models using data that consists only of *feature* values without any known labels. Unsupervised machine learning algorithms determine relationships be...",
    "keywords": [
      "unsupervised",
      "machine",
      "learning",
      "training",
      "data",
      "involves",
      "models",
      "consists",
      "feature",
      "values",
      "labels",
      "algorithms",
      "analytics",
      "algorithm",
      "prediction",
      "model",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 18,
    "category": "Machine Learning",
    "file": "02-types-of-machine-learning.md",
    "heading": "Clustering",
    "content": "The most common form of unsupervised machine learning is *clustering*. A clustering algorithm identifies similarities between observations based on their features, and groups them into discrete clusters. For example:\n\n- Group similar flowers based on their size, number of leaves, and number of petals.\n- Identify groups of similar customers based on demographic attributes and purchasing behavior.\n\nIn some ways, clustering is similar to multiclass classification; in that it categorizes observation",
    "summary": "The most common form of unsupervised machine learning is *clustering*. A clustering algorithm identifies similarities between observations based on their features, and groups them into discrete cluste...",
    "keywords": [
      "clustering",
      "groups",
      "observations",
      "classification",
      "based",
      "algorithm",
      "features",
      "similar",
      "classes",
      "data",
      "label",
      "customer",
      "analytics",
      "segmentation",
      "categorization",
      "field",
      "prediction",
      "model",
      "grouping",
      "extraction",
      "sorting",
      "training",
      "labeling",
      "unsupervised",
      "structured-data",
      "patterns",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 19,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Example - regression",
    "content": "Let's explore regression with a simplified example in which we'll train a model to predict a numeric label (***y***) based on a single feature value (***x***). Most real scenarios involve multiple feature values, which adds some complexity; but the principle is the same.\n\nFor our example, let's stick with the ice cream sales scenario we discussed previously. For our feature, we'll consider the *temperature* (let's assume the value is the maximum temperature on a given day), and the label we want",
    "summary": "Let's explore regression with a simplified example in which we'll train a model to predict a numeric label (***y***) based on a single feature value (***x***). Most real scenarios involve multiple fea...",
    "keywords": [
      "temperature",
      "example",
      "feature",
      "cream",
      "sales",
      "regression",
      "train",
      "model",
      "predict",
      "label",
      "value",
      "creams",
      "analytics",
      "algorithm",
      "prediction",
      "continuous",
      "forecasting",
      "training",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 20,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Training a regression model",
    "content": "We'll start by splitting the data and using a subset of it to train a model. Here's the training dataset:\n\n|Temperature (x) | Ice cream sales (y)|\n|---|---|\n|51|1|\n|65|14|\n|69|20|\n|72|23|\n|75|26|\n|81|30|\n\nTo get an insight of how these ***x*** and ***y*** values might relate to one another, we can plot them as coordinates along two axes, like this:\n\n![Diagram of a scatter plot showing x and y.](../media/scatter-plot.png)\n\nNow we're ready to apply an algorithm to our training data and fit it to a",
    "summary": "We'll start by splitting the data and using a subset of it to train a model. Here's the training dataset:\n\n|Temperature (x) | Ice cream sales (y)|\n|---|---|\n|51|1|\n|65|14|\n|69|20|\n|72|23|\n|75|26|\n|81|...",
    "keywords": [
      "line",
      "function",
      "model",
      "plot",
      "calculate",
      "value",
      "axis",
      "training",
      "regression",
      "given",
      "data",
      "temperature",
      "analytics",
      "search",
      "algorithm",
      "retrieval",
      "grounding",
      "prediction",
      "continuous",
      "forecasting",
      "estimation",
      "data-science",
      "ml",
      "context"
    ]
  },
  {
    "id": 21,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Evaluating a regression model",
    "content": "To validate the model and evaluate how well it predicts, we held back some data for which we know the label (***y***) value. Here's the data we held back:\n\n|Temperature (x) | Ice cream sales (y)|\n|---|---|\n|52|0|\n|67|14|\n|70|23|\n|73|22|\n|78|26|\n|83|36|\n\nWe can use the model to predict the label for each of the observations in this dataset based on the feature (***x***) value; and then compare the predicted label (***&#375;***) to the known actual label value (***y***).\n\nUsing the model we traine",
    "summary": "To validate the model and evaluate how well it predicts, we held back some data for which we know the label (***y***) value. Here's the data we held back:\n\n|Temperature (x) | Ice cream sales (y)|\n|---...",
    "keywords": [
      "actual",
      "model",
      "predicted",
      "values",
      "label",
      "value",
      "sales",
      "function",
      "plot",
      "held",
      "data",
      "temperature",
      "analytics",
      "algorithm",
      "prediction",
      "continuous",
      "forecasting",
      "training",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 22,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Regression evaluation metrics",
    "content": "Based on the differences between the predicted and actual values, you can calculate some common metrics that are used to evaluate a regression model.",
    "summary": "Based on the differences between the predicted and actual values, you can calculate some common metrics that are used to evaluate a regression model.",
    "keywords": [
      "regression",
      "metrics",
      "evaluation",
      "based",
      "differences",
      "predicted",
      "actual",
      "values",
      "calculate",
      "common",
      "evaluate",
      "model",
      "analytics",
      "algorithm",
      "prediction",
      "continuous",
      "forecasting",
      "training",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 23,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Mean Absolute Error (MAE)",
    "content": "The variance in this example indicates by how many ice creams each prediction was wrong. It doesn't matter if the prediction was *over* or *under* the actual value (so for example, -3 and +3 both indicate a variance of 3). This metric is known as the *absolute error* for each prediction, and can be summarized for the whole validation set as the **mean absolute error** (MAE).\n\nIn the ice cream example, the mean (average) of the absolute errors (2, 3, 3, 1, 2, and 3) is **2.33**.",
    "summary": "The variance in this example indicates by how many ice creams each prediction was wrong. It doesn't matter if the prediction was *over* or *under* the actual value (so for example, -3 and +3 both indi...",
    "keywords": [
      "absolute",
      "mean",
      "error",
      "example",
      "prediction",
      "variance",
      "indicates",
      "creams",
      "wrong",
      "doesn",
      "matter",
      "actual",
      "analytics",
      "search",
      "algorithm",
      "retrieval",
      "grounding",
      "model",
      "training",
      "data-science",
      "ml",
      "context"
    ]
  },
  {
    "id": 24,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Mean Squared Error (MSE)",
    "content": "The mean absolute error metric takes all discrepancies between predicted and actual labels into account equally. However, it may be more desirable to have a model that is consistently wrong by a small amount than one that makes fewer, but larger errors. One way to produce a metric that \"amplifies\" larger errors by *squaring* the individual errors and calculating the mean of the squared values. This metric is known as the **mean squared error** (MSE).\n\nIn our ice cream example, the mean of the sq",
    "summary": "The mean absolute error metric takes all discrepancies between predicted and actual labels into account equally. However, it may be more desirable to have a model that is consistently wrong by a small...",
    "keywords": [
      "mean",
      "squared",
      "error",
      "metric",
      "errors",
      "absolute",
      "larger",
      "values",
      "discrepancies",
      "predicted",
      "actual",
      "labels",
      "analytics",
      "algorithm",
      "prediction",
      "model",
      "training",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 25,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Root Mean Squared Error (RMSE)",
    "content": "The mean squared error helps take the magnitude of errors into account, but because it *squares* the error values, the resulting metric no longer represents the quantity measured by the label. In other words, we can say that the MSE of our model is 6, but that doesn't measure its accuracy in terms of the number of ice creams that were mispredicted; 6 is just a numeric score that indicates the level of error in the validation predictions.\n\nIf we want to measure the error in terms of the number of",
    "summary": "The mean squared error helps take the magnitude of errors into account, but because it *squares* the error values, the resulting metric no longer represents the quantity measured by the label. In othe...",
    "keywords": [
      "error",
      "root",
      "mean",
      "squared",
      "creams",
      "metric",
      "measure",
      "terms",
      "number",
      "rmse",
      "helps",
      "magnitude",
      "analytics",
      "algorithm",
      "prediction",
      "model",
      "training",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 26,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Coefficient of determination (R<sup>2</sup>)",
    "content": "All of the metrics so far compare the discrepancy between the predicted and actual values in order to evaluate the model. However, in reality, there's some natural random variance in the daily sales of ice cream that the model takes into account. In a linear regression model, the training algorithm fits a straight line that minimizes the mean variance between the function and the known label values. The **coefficient of determination** (more commonly referred to as **R<sup>2</sup>** or **R-Squar",
    "summary": "All of the metrics so far compare the discrepancy between the predicted and actual values in order to evaluate the model. However, in reality, there's some natural random variance in the daily sales o...",
    "keywords": [
      "model",
      "actual",
      "values",
      "variance",
      "validation",
      "label",
      "data",
      "coefficient",
      "determination",
      "metrics",
      "predicted",
      "sales",
      "analytics",
      "algorithm",
      "prediction",
      "continuous",
      "forecasting",
      "training",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 27,
    "category": "Machine Learning",
    "file": "03-regression.md",
    "heading": "Iterative training",
    "content": "The metrics described above are commonly used to evaluate a regression model. In most real-world scenarios, a data scientist will use an iterative process to repeatedly train and evaluate a model, varying:\n\n- Feature selection and preparation (choosing which features to include in the model, and calculations applied to them to help ensure a better fit).\n- Algorithm selection (We explored linear regression in the previous example, but there are many other regression algorithms)\n- Algorithm parame",
    "summary": "The metrics described above are commonly used to evaluate a regression model. In most real-world scenarios, a data scientist will use an iterative process to repeatedly train and evaluate a model, var...",
    "keywords": [
      "model",
      "regression",
      "algorithm",
      "iterative",
      "evaluate",
      "selection",
      "parameters",
      "training",
      "metrics",
      "described",
      "commonly",
      "real-world",
      "analytics",
      "prediction",
      "continuous",
      "forecasting",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 28,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Example - binary classification",
    "content": "To understand how binary classification works, let's look at a simplified example that uses a single feature (***x***) to predict whether the label ***y*** is 1 or 0. In this example, we'll use the blood glucose level of a patient to predict whether or not the patient has diabetes. Here's the data with which we'll train the model:\n\n|![Diagram of a syringe.](../media/blood-glucose.png)|![Diagram of a diabetic and non-diabetic person.](../media/diabetes.png)|\n|---|---|\n|**Blood glucose (x)** | **D",
    "summary": "To understand how binary classification works, let's look at a simplified example that uses a single feature (***x***) to predict whether the label ***y*** is 1 or 0. In this example, we'll use the bl...",
    "keywords": [
      "example",
      "binary",
      "classification",
      "predict",
      "blood",
      "glucose",
      "patient",
      "diabetes",
      "diagram",
      "media",
      "diabetic",
      "understand",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "model",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 29,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Training a binary classification model",
    "content": "To train the model, we'll use an algorithm to fit the training data to a function that calculates the *probability* of the class label being *true* (in other words, that the patient has diabetes). Probability is measured as a value between 0.0 and 1.0, such that the *total* probability for *all* possible classes is 1.0. So for example, if the probability of a patient having diabetes is 0.7, then there's a corresponding probability of 0.3 that the patient isn't</u> diabetic.\n\nThere are many algor",
    "summary": "To train the model, we'll use an algorithm to fit the training data to a function that calculates the *probability* of the class label being *true* (in other words, that the patient has diabetes). Pro...",
    "keywords": [
      "probability",
      "function",
      "true",
      "model",
      "patient",
      "value",
      "predict",
      "training",
      "diabetes",
      "values",
      "false",
      "threshold",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "continuous",
      "forecasting",
      "sorting",
      "labeling",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 30,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Evaluating a binary classification model",
    "content": "As with regression, when training a binary classification model you hold back a random subset of data with which to validate the trained model. Let's assume we held back the following data to validate our diabetes classifier:\n\n|Blood glucose (x) | Diabetic? (y)|\n|---|---|\n|66|0|\n|107|1|\n|112|1|\n|71|0|\n|87|1|\n|89|1|\n\nApplying the logistic function we derived previously to the ***x*** values results in the following plot. \n\n![Diagram of predicted labels on a sigmoid curve.](../media/classification",
    "summary": "As with regression, when training a binary classification model you hold back a random subset of data with which to validate the trained model. Let's assume we held back the following data to validate...",
    "keywords": [
      "model",
      "predicted",
      "diabetes",
      "labels",
      "binary",
      "classification",
      "data",
      "validate",
      "following",
      "blood",
      "glucose",
      "function",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "continuous",
      "forecasting",
      "sorting",
      "training",
      "labeling",
      "estimation",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 31,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Binary classification evaluation metrics",
    "content": "The first step in calculating evaluation metrics for a binary classification model is usually to create a matrix of the number of correct and incorrect predictions for each possible class label:\n\n![Diagram of a confusion matrix.](../media/binary-confusion-matrix.png)\n\nThis visualization is called a *confusion matrix*, and it shows the prediction totals where:\n\n- &#375;=0 and y=0: *True negatives* (TN)\n- &#375;=1 and y=0: *False positives* (FP)\n- &#375;=0 and y=1: *False negatives* (FN)\n- &#375;=",
    "summary": "The first step in calculating evaluation metrics for a binary classification model is usually to create a matrix of the number of correct and incorrect predictions for each possible class label:\n\n![Di...",
    "keywords": [
      "matrix",
      "predictions",
      "confusion",
      "true",
      "binary",
      "classification",
      "evaluation",
      "metrics",
      "model",
      "number",
      "correct",
      "negatives",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 32,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Accuracy",
    "content": "The simplest metric you can calculate from the confusion matrix is *accuracy* - the proportion of predictions that the model got right. Accuracy is calculated as:\n\n***(TN+TP) &#247; (TN+FN+FP+TP)***\n\nIn the case of our diabetes example, the calculation is:\n\n(2+3) &#247; (2+1+0+3)\n\n= 5 &#247; 6\n\n= **0.83**\n\nSo for our validation data, the diabetes classification model produced correct predictions 83% of the time.\n\nAccuracy might initially seem like a good metric to evaluate a model, but consider ",
    "summary": "The simplest metric you can calculate from the confusion matrix is *accuracy* - the proportion of predictions that the model got right. Accuracy is calculated as:\n\n***(TN+TP) &#247; (TN+FN+FP+TP)***...",
    "keywords": [
      "accuracy",
      "model",
      "diabetes",
      "metric",
      "predictions",
      "cases",
      "simplest",
      "calculate",
      "confusion",
      "matrix",
      "proportion",
      "right",
      "analytics",
      "algorithm",
      "categorization",
      "field",
      "prediction",
      "extraction",
      "sorting",
      "training",
      "labeling",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 33,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Recall",
    "content": "*Recall* is a metric that measures the proportion of positive cases that the model identified correctly. In other words, compared to the number of patients who *have* diabetes, how many did the model *predict* to have diabetes?\n\nThe formula for recall is:\n\n***TP &#247; (TP+FN)***\n\nFor our diabetes example:\n\n3 &#247; (3+1)\n\n= 3 &#247; 4\n\n= **0.75**\n\nSo our model correctly identified 75% of patients who have diabetes as having diabetes.",
    "summary": "*Recall* is a metric that measures the proportion of positive cases that the model identified correctly. In other words, compared to the number of patients who *have* diabetes, how many did the model...",
    "keywords": [
      "diabetes",
      "recall",
      "model",
      "identified",
      "correctly",
      "patients",
      "metric",
      "measures",
      "proportion",
      "positive",
      "cases",
      "words",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "extraction",
      "training",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 34,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Precision",
    "content": "*Precision* is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. In other words, what proportion of the patients *predicted* by the model to have diabetes actually *have* diabetes?\n\nThe formula for precision is:\n\n***TP &#247; (TP+FP)***\n\nFor our diabetes example:\n\n3 &#247; (3+0)\n\n= 3 &#247; 3\n\n= **1.0**\n\nSo 100% of the patients predicted by our model to have diabetes do in fact have diabetes.",
    "summary": "*Precision* is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. In other words, what proportion of the patients *predicted...",
    "keywords": [
      "diabetes",
      "precision",
      "predicted",
      "proportion",
      "positive",
      "actually",
      "patients",
      "model",
      "similar",
      "metric",
      "recall",
      "measures",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "extraction",
      "training",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 35,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "F1-score",
    "content": "*F1-score* is an overall metric that combines recall and precision. The formula for F1-score is:\n\n***(2 x Precision x Recall) &#247; (Precision + Recall)***\n\nFor our diabetes example:\n\n(2 x 1.0 x 0.75) &#247; (1.0 + 0.75)\n\n= 1.5 &#247; 1.75\n\n**= 0.86**",
    "summary": "*F1-score* is an overall metric that combines recall and precision. The formula for F1-score is:\n\n***(2 x Precision x Recall) &#247; (Precision + Recall)***\n\nFor our diabetes example:\n\n(2 x 1.0 x 0.75...",
    "keywords": [
      "f1-score",
      "recall",
      "precision",
      "overall",
      "metric",
      "combines",
      "formula",
      "diabetes",
      "example",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "model",
      "extraction",
      "training",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 36,
    "category": "Machine Learning",
    "file": "04-binary-classification.md",
    "heading": "Area Under the Curve (AUC)",
    "content": "Another name for recall is the *true positive rate* (TPR), and there's an equivalent metric called the *false positive rate* (FPR) that is calculated as **FP&#247;(FP+TN)**. We already know that the TPR for our model when using a threshold of 0.5 is 0.75, and we can use the formula for FPR to calculate a value of 0&#247;2 = 0.\n\nOf course, if we were to change the threshold above which the model predicts *true* (**1**), it would affect the number of positive and negative predictions; and therefor",
    "summary": "Another name for recall is the *true positive rate* (TPR), and there's an equivalent metric called the *false positive rate* (FPR) that is calculated as **FP&#247;(FP+TN)**. We already know that the T...",
    "keywords": [
      "curve",
      "model",
      "area",
      "positive",
      "threshold",
      "true",
      "rate",
      "metric",
      "value",
      "change",
      "metrics",
      "possible",
      "analytics",
      "algorithm",
      "field",
      "prediction",
      "extraction",
      "training",
      "structured-data",
      "data-science",
      "ml",
      "template"
    ]
  },
  {
    "id": 37,
    "category": "Machine Learning",
    "file": "05-multiclass-classification.md",
    "heading": "Example - multiclass classification",
    "content": "Multiclass classification algorithms are used to calculate probability values for multiple class labels, enabling a model to predict the *most probable* class for a given observation.\n\nLet's explore an example in which we have some observations of penguins, in which the flipper length (***x***) of each penguin is recorded. For each observation, the data includes the penguin species (***y***), which is encoded as follows:\n\n- 0: Adelie\n- 1: Gentoo\n- 2: Chinstrap\n\n\n\n|![Diagram of a measuring ruler.",
    "summary": "Multiclass classification algorithms are used to calculate probability values for multiple class labels, enabling a model to predict the *most probable* class for a given observation.\n\nLet's explore a...",
    "keywords": [
      "penguins",
      "example",
      "multiclass",
      "classification",
      "class",
      "observation",
      "flipper",
      "length",
      "penguin",
      "species",
      "diagram",
      "ruler",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "model",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 38,
    "category": "Machine Learning",
    "file": "05-multiclass-classification.md",
    "heading": "Training a multiclass classification model",
    "content": "To train a multiclass classification model, we need to use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this:\n\n- One-vs-Rest (OvR) algorithms\n- Multinomial algorithms",
    "summary": "To train a multiclass classification model, we need to use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algor...",
    "keywords": [
      "training",
      "multiclass",
      "classification",
      "model",
      "algorithm",
      "algorithms",
      "train",
      "data",
      "function",
      "calculates",
      "probability",
      "value",
      "analytics",
      "categorization",
      "prediction",
      "sorting",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 39,
    "category": "Machine Learning",
    "file": "05-multiclass-classification.md",
    "heading": "One-vs-Rest (OvR) algorithms",
    "content": "One-vs-Rest algorithms train a binary classification function for each class, each calculating the probability that the observation is an example of the target class. Each function calculates the probability of the observation being a specific class compared to *any* other class. For our penguin species classification model, the algorithm would essentially create three binary classification functions:\n\n- ***f<sup>0</sup>(x) = P(y=0 | x)***\n- ***f<sup>1</sup>(x) = P(y=1 | x)***\n- ***f<sup>2</sup>",
    "summary": "One-vs-Rest algorithms train a binary classification function for each class, each calculating the probability that the observation is an example of the target class. Each function calculates the prob...",
    "keywords": [
      "class",
      "function",
      "probability",
      "classification",
      "algorithm",
      "one-vs-rest",
      "algorithms",
      "binary",
      "observation",
      "calculates",
      "model",
      "produces",
      "analytics",
      "categorization",
      "prediction",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 40,
    "category": "Machine Learning",
    "file": "05-multiclass-classification.md",
    "heading": "Multinomial algorithms",
    "content": "As an alternative approach is to use a multinomial algorithm, which creates a single function that returns a multi-valued output. The output is a *vector* (an array of values) that contains the *probability distribution* for all possible classes - with a probability score for each class which when totaled add up to 1.0:\n\n***f(x) =[P(y=0|x), P(y=1|x), P(y=2|x)]***\n\nAn example of this kind of function is a *softmax* function, which could produce an output like the following example:\n\n[0.2, 0.3, 0.",
    "summary": "As an alternative approach is to use a multinomial algorithm, which creates a single function that returns a multi-valued output. The output is a *vector* (an array of values) that contains the *proba...",
    "keywords": [
      "function",
      "class",
      "output",
      "probability",
      "multinomial",
      "algorithm",
      "vector",
      "classes",
      "example",
      "algorithms",
      "alternative",
      "approach",
      "analytics",
      "prediction",
      "model",
      "training",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 41,
    "category": "Machine Learning",
    "file": "05-multiclass-classification.md",
    "heading": "Evaluating a multiclass classification model",
    "content": "You can evaluate a multiclass classifier by calculating binary classification metrics for each individual class. Alternatively, you can calculate aggregate metrics that take all classes into account.\n\nLet's assume that we've validated our multiclass classifier, and obtained the following results:\n\n|Flipper length (x) | Actual species (y)| Predicted species (&#375;)|\n|---|---|--|\n|165|0|0|\n|171|0|0|\n|205|2|1|\n|195|1|1|\n|183|1|1|\n|221|2|2|\n|214|2|2|\n\nThe confusion matrix for a multiclass classifie",
    "summary": "You can evaluate a multiclass classifier by calculating binary classification metrics for each individual class. Alternatively, you can calculate aggregate metrics that take all classes into account....",
    "keywords": [
      "overall",
      "metrics",
      "multiclass",
      "classifier",
      "class",
      "recall",
      "precision",
      "confusion",
      "matrix",
      "accuracy",
      "f1-score",
      "classification",
      "analytics",
      "algorithm",
      "categorization",
      "prediction",
      "model",
      "sorting",
      "training",
      "labeling",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 42,
    "category": "Machine Learning",
    "file": "06-clustering.md",
    "heading": "Example - clustering",
    "content": "For example, suppose a botanist observes a sample of flowers and records the number of leaves and petals on each flower:\n\n![Diagram of some flowers.](../media/flowers.png)\n\nThere are no known *labels* in the dataset, just two *features*. The goal is not to identify the different types (species) of flower; just to group similar flowers together based on the number of leaves and petals.\n\n|Leaves *(x<sub>1</sub>)*| Petals *(x<sub>2</sub>)*|\n|-|-|\n|0|5|\n|0|6|\n|1|3|\n|1|3|\n|1|6|\n|1|8|\n|2|3|\n|2|7|\n|2|8",
    "summary": "For example, suppose a botanist observes a sample of flowers and records the number of leaves and petals on each flower:\n\n![Diagram of some flowers.](../media/flowers.png)\n\nThere are no known *labels*...",
    "keywords": [
      "flowers",
      "leaves",
      "petals",
      "example",
      "number",
      "flower",
      "clustering",
      "suppose",
      "botanist",
      "observes",
      "sample",
      "records",
      "analytics",
      "segmentation",
      "algorithm",
      "prediction",
      "model",
      "grouping",
      "training",
      "unsupervised",
      "patterns",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 43,
    "category": "Machine Learning",
    "file": "06-clustering.md",
    "heading": "Training a clustering model",
    "content": "There are multiple algorithms you can use for clustering. One of the most commonly used algorithms is *K-Means* clustering, which consists of the following steps:\n\n1. The feature (***x***) values are vectorized to define *n*-dimensional coordinates (where *n* is the number of features). In the flower example, we have two features: number of leaves (***x<sub>1</sub>***) and number of petals (***x<sub>2</sub>***). So, the feature vector has two coordinates that we can use to conceptually plot the ",
    "summary": "There are multiple algorithms you can use for clustering. One of the most commonly used algorithms is *K-Means* clustering, which consists of the following steps:\n\n1. The feature (***x***) values are...",
    "keywords": [
      "points",
      "centroid",
      "clustering",
      "data",
      "number",
      "clusters",
      "coordinates",
      "algorithms",
      "k-means",
      "following",
      "steps",
      "feature",
      "analytics",
      "segmentation",
      "algorithm",
      "prediction",
      "model",
      "grouping",
      "training",
      "unsupervised",
      "patterns",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 44,
    "category": "Machine Learning",
    "file": "06-clustering.md",
    "heading": "Evaluating a clustering model",
    "content": "Since there's no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.\n\nThere are multiple metrics that you can use to evaluate cluster separation, including:\n\n- **Average distance to cluster center**: How close, on average, each point in the cluster is to the centroid of the cluster.\n- **Average distance to other center**: How close, on average, each point in the cluster i",
    "summary": "Since there's no known label with which to compare the predicted cluster assignments, evaluation of a clustering model is based on how well the resulting clusters are separated from one another.\n\nTher...",
    "keywords": [
      "cluster",
      "distance",
      "average",
      "clusters",
      "center",
      "point",
      "centroid",
      "clustering",
      "model",
      "separation",
      "close",
      "points",
      "analytics",
      "segmentation",
      "algorithm",
      "retrieval",
      "search",
      "grounding",
      "prediction",
      "grouping",
      "training",
      "unsupervised",
      "patterns",
      "data-science",
      "ml",
      "context"
    ]
  },
  {
    "id": 45,
    "category": "Machine Learning",
    "file": "07-deep-learning.md",
    "heading": "Example - Using deep learning for classification",
    "content": "To better understand how a deep neural network model works, let's explore an example in which a neural network is used to define a classification model for penguin species.\n\n![Diagram of a neural network used to classify a penguin species.](../media/deep-classification.png)\n\nThe feature data (***x***) consists of some measurements of a penguin. Specifically, the measurements are:\n\n- The length of the penguin's bill.\n- The depth of the penguin's bill.\n- The length of the penguin's flippers.\n- The",
    "summary": "To better understand how a deep neural network model works, let's explore an example in which a neural network is used to define a classification model for penguin species.\n\n![Diagram of a neural netw...",
    "keywords": [
      "penguin",
      "layer",
      "network",
      "vector",
      "example",
      "model",
      "species",
      "classification",
      "neural",
      "consists",
      "values",
      "three",
      "analytics",
      "algorithm",
      "neurons",
      "categorization",
      "cnn",
      "rnn",
      "prediction",
      "neural-networks",
      "transformer",
      "sorting",
      "training",
      "labeling",
      "layers",
      "backpropagation",
      "deep-learning",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 46,
    "category": "Machine Learning",
    "file": "07-deep-learning.md",
    "heading": "How does a neural network learn?",
    "content": "The weights in a neural network are central to how it calculates predicted values for labels. During the training process, the model *learns* the weights that will result in the most accurate predictions. Let's explore the training process in a little more detail to understand how this learning takes place.\n\n![Diagram of a neural network being trained, evaluated, and optimized.](../media/neural-network-training.png)\n\n1. The training and validation datasets are defined, and the training features ",
    "summary": "The weights in a neural network are central to how it calculates predicted values for labels. During the training process, the model *learns* the weights that will result in the most accurate predicti...",
    "keywords": [
      "network",
      "loss",
      "values",
      "weights",
      "training",
      "function",
      "neural",
      "process",
      "layer",
      "output",
      "calculates",
      "predicted",
      "analytics",
      "algorithm",
      "neurons",
      "prediction",
      "model",
      "layers",
      "backpropagation",
      "deep-learning",
      "data-science",
      "ml"
    ]
  },
  {
    "id": 47,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Common speech recognition scenarios",
    "content": "Speech recognition, also called speech-to-text, listens to audio input and transcribes it into written text. This capability powers a wide range of business and consumer applications.",
    "summary": "Speech recognition, also called speech-to-text, listens to audio input and transcribes it into written text. This capability powers a wide range of business and consumer applications.",
    "keywords": [
      "speech",
      "recognition",
      "common",
      "scenarios",
      "called",
      "speech-to-text",
      "listens",
      "audio",
      "input",
      "transcribes",
      "written",
      "text",
      "stt",
      "pronunciation",
      "listening",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 48,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Customer service and support",
    "content": "Service centers use speech recognition to:\n\n- Transcribe customer calls in real time for agent reference and quality assurance.\n- Route callers to the right department based on what they say.\n- Analyze call sentiment and identify common customer issues.\n- Generate searchable call records for compliance and training.\n\n**Business value**: Reduces manual note-taking, improves response accuracy, and captures insights that improve service quality.",
    "summary": "Service centers use speech recognition to:\n\n- Transcribe customer calls in real time for agent reference and quality assurance.\n- Route callers to the right department based on what they say.\n- Analyz...",
    "keywords": [
      "customer",
      "service",
      "quality",
      "call",
      "support",
      "centers",
      "speech",
      "recognition",
      "transcribe",
      "calls",
      "real",
      "time",
      "stt",
      "pronunciation",
      "planning",
      "listening",
      "autonomous",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "tool-use",
      "recording",
      "function-calling",
      "microphone",
      "speech-to-text"
    ]
  },
  {
    "id": 49,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Voice-activated assistants and agents",
    "content": "Virtual assistants and AI agents rely on speech recognition to:\n\n- Accept voice commands for hands-free control of devices and applications.\n- Answer questions using natural language understanding.\n- Complete tasks like setting reminders, sending messages, or searching information.\n- Control smart home devices, automotive systems, and wearable technology.\n\n**Business value**: Increases user engagement, simplifies complex workflows, and enables operation in situations where screens aren't practic",
    "summary": "Virtual assistants and AI agents rely on speech recognition to:\n\n- Accept voice commands for hands-free control of devices and applications.\n- Answer questions using natural language understanding.\n-...",
    "keywords": [
      "assistants",
      "agents",
      "control",
      "devices",
      "voice-activated",
      "virtual",
      "rely",
      "speech",
      "recognition",
      "accept",
      "voice",
      "commands",
      "stt",
      "extraction",
      "microphone",
      "artificial-intelligence",
      "field",
      "audio",
      "voice-input",
      "speaking",
      "function-calling",
      "dictation",
      "tool-use",
      "recording",
      "speech-to-text",
      "template",
      "transcription",
      "pronunciation",
      "planning",
      "listening",
      "autonomous",
      "sound",
      "ai",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 50,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Meeting and interview transcription",
    "content": "Organizations transcribe conversations to:\n\n- Create searchable meeting notes and action item lists.\n- Provide real-time captions for participants who are deaf or hard of hearing.\n- Generate summaries of interviews, focus groups, and research sessions.\n- Extract key discussion points for documentation and follow-up.\n\n**Business value**: Saves hours of manual transcription work, ensures accurate records, and makes spoken content accessible to everyone.",
    "summary": "Organizations transcribe conversations to:\n\n- Create searchable meeting notes and action item lists.\n- Provide real-time captions for participants who are deaf or hard of hearing.\n- Generate summaries...",
    "keywords": [
      "meeting",
      "transcription",
      "interview",
      "organizations",
      "transcribe",
      "conversations",
      "create",
      "searchable",
      "notes",
      "action",
      "item",
      "lists",
      "pronunciation",
      "listening",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 51,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Healthcare documentation",
    "content": "Clinical professionals use speech recognition to:\n\n- Dictate patient notes directly into electronic health records.\n- Update treatment plans without interrupting patient care.\n- Reduce administrative burden and prevent physician burnout.\n- Improve documentation accuracy by capturing details in the moment.\n\n**Business value**: Increases time available for patient care, improves record completeness, and reduces documentation errors.",
    "summary": "Clinical professionals use speech recognition to:\n\n- Dictate patient notes directly into electronic health records.\n- Update treatment plans without interrupting patient care.\n- Reduce administrative...",
    "keywords": [
      "documentation",
      "patient",
      "care",
      "healthcare",
      "clinical",
      "professionals",
      "speech",
      "recognition",
      "dictate",
      "notes",
      "directly",
      "electronic",
      "stt",
      "pronunciation",
      "listening",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "recording",
      "microphone",
      "speech-to-text"
    ]
  },
  {
    "id": 52,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Common speech synthesis scenarios",
    "content": "Speech synthesis, also called text-to-speech, converts written text into spoken audio. This technology creates voices for applications that need to communicate information audibly.",
    "summary": "Speech synthesis, also called text-to-speech, converts written text into spoken audio. This technology creates voices for applications that need to communicate information audibly.",
    "keywords": [
      "speech",
      "synthesis",
      "common",
      "scenarios",
      "called",
      "text-to-speech",
      "converts",
      "written",
      "text",
      "spoken",
      "audio",
      "technology",
      "extraction",
      "narration",
      "microphone",
      "field",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "pronunciation",
      "listening",
      "tts",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 53,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Conversational AI and chatbots",
    "content": "AI agents use speech synthesis to:\n\n- Respond to users with natural-sounding voices instead of requiring them to read text.\n- Create personalized interactions by adjusting tone, pace, and speaking style.\n- Handle customer inquiries through voice channels like phone systems.\n- Provide consistent brand experiences across voice and text interfaces.\n\n**Business value**: Makes AI agents more approachable, reduces customer effort, and extends service availability to voice-only channels.",
    "summary": "AI agents use speech synthesis to:\n\n- Respond to users with natural-sounding voices instead of requiring them to read text.\n- Create personalized interactions by adjusting tone, pace, and speaking sty...",
    "keywords": [
      "agents",
      "text",
      "customer",
      "voice",
      "channels",
      "conversational",
      "chatbots",
      "speech",
      "synthesis",
      "respond",
      "users",
      "natural-sounding",
      "narration",
      "microphone",
      "artificial-intelligence",
      "audio",
      "speaking",
      "function-calling",
      "dictation",
      "voice-generation",
      "recording",
      "tool-use",
      "pronunciation",
      "planning",
      "listening",
      "autonomous",
      "tts",
      "reading-aloud",
      "sound",
      "ai",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 54,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Accessibility and content consumption",
    "content": "Applications generate audio to:\n\n- Read web content, articles, and documents aloud for users with visual impairments.\n- Support users with reading disabilities like dyslexia.\n- Enable content consumption while driving, exercising, or performing other tasks.\n- Provide audio alternatives for text-heavy interfaces.\n\n**Business value**: Expands your audience reach, demonstrates commitment to inclusion, and improves user satisfaction.",
    "summary": "Applications generate audio to:\n\n- Read web content, articles, and documents aloud for users with visual impairments.\n- Support users with reading disabilities like dyslexia.\n- Enable content consumpt...",
    "keywords": [
      "content",
      "consumption",
      "audio",
      "users",
      "accessibility",
      "applications",
      "generate",
      "read",
      "articles",
      "documents",
      "aloud",
      "visual",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "structured-data",
      "accent",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 55,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Notifications and alerts",
    "content": "Systems use speech synthesis to:\n\n- Announce important alerts, reminders, and status updates.\n- Provide navigation instructions in mapping and GPS applications.\n- Deliver time-sensitive information without requiring users to look at screens.\n- Communicate system status in industrial and operational environments.\n\n**Business value**: Ensures critical information reaches users even when visual attention isn't available, improving safety and responsiveness.",
    "summary": "Systems use speech synthesis to:\n\n- Announce important alerts, reminders, and status updates.\n- Provide navigation instructions in mapping and GPS applications.\n- Deliver time-sensitive information wi...",
    "keywords": [
      "alerts",
      "status",
      "information",
      "users",
      "notifications",
      "systems",
      "speech",
      "synthesis",
      "announce",
      "important",
      "reminders",
      "updates",
      "extraction",
      "narration",
      "microphone",
      "field",
      "audio",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "pronunciation",
      "listening",
      "tts",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 56,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "E-learning and training",
    "content": "Educational platforms use speech synthesis to:\n\n- Create narrated lessons and course content without recording studios.\n- Provide pronunciation examples for language learning.\n- Generate audio versions of written materials for different learning preferences.\n- Scale content production across multiple languages.\n\n**Business value**: Reduces content creation costs, supports diverse learning styles, and accelerates course development timelines.",
    "summary": "Educational platforms use speech synthesis to:\n\n- Create narrated lessons and course content without recording studios.\n- Provide pronunciation examples for language learning.\n- Generate audio version...",
    "keywords": [
      "content",
      "learning",
      "course",
      "e-learning",
      "training",
      "educational",
      "platforms",
      "speech",
      "synthesis",
      "create",
      "narrated",
      "lessons",
      "extraction",
      "narration",
      "microphone",
      "field",
      "audio",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "pronunciation",
      "listening",
      "tts",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 57,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Entertainment and media",
    "content": "Content creators use speech synthesis to:\n\n- Generate character voices for games and interactive experiences.\n- Produce podcast drafts and audiobook prototypes.\n- Create voiceovers for videos and presentations.\n- Personalize audio content based on user preferences.\n\n**Business value**: Lowers production costs, enables rapid prototyping, and creates customized experiences at scale.",
    "summary": "Content creators use speech synthesis to:\n\n- Generate character voices for games and interactive experiences.\n- Produce podcast drafts and audiobook prototypes.\n- Create voiceovers for videos and pres...",
    "keywords": [
      "content",
      "experiences",
      "entertainment",
      "media",
      "creators",
      "speech",
      "synthesis",
      "generate",
      "character",
      "voices",
      "games",
      "interactive",
      "pronunciation",
      "listening",
      "tts",
      "narration",
      "reading-aloud",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "accent",
      "voice-generation",
      "recording",
      "text-to-speech",
      "microphone"
    ]
  },
  {
    "id": 58,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Combining speech recognition and synthesis",
    "content": "The most powerful speech-enabled applications combine both capabilities to create conversational experiences:\n\n- **Voice-driven customer service**: Agents listen to customer questions (recognition), process the request, and respond with helpful answers (synthesis).\n- **Interactive voice response (IVR) systems**: Callers speak their needs, and the system guides them through options using natural dialogue.\n- **Language learning applications**: Students speak practice phrases (recognition), and the",
    "summary": "The most powerful speech-enabled applications combine both capabilities to create conversational experiences:\n\n- **Voice-driven customer service**: Agents listen to customer questions (recognition), p...",
    "keywords": [
      "recognition",
      "synthesis",
      "system",
      "applications",
      "create",
      "customer",
      "speak",
      "natural",
      "combining",
      "speech",
      "powerful",
      "speech-enabled",
      "stt",
      "pronunciation",
      "planning",
      "listening",
      "autonomous",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "tool-use",
      "recording",
      "function-calling",
      "microphone",
      "speech-to-text"
    ]
  },
  {
    "id": 59,
    "category": "Speech",
    "file": "01-speech-solutions.md",
    "heading": "Key considerations before implementing speech",
    "content": "Before you add speech capabilities to your application, evaluate these factors:\n\n- **Audio quality requirements**: Background noise, microphone quality, and network bandwidth affect speech recognition accuracy.\n- **Language and dialect support**: Verify that your target languages and regional variations are supported.\n- **Privacy and compliance**: Understand how audio data is processed, stored, and protected to meet regulatory requirements.\n- **Latency expectations**: Real-time conversations req",
    "summary": "Before you add speech capabilities to your application, evaluate these factors:\n\n- **Audio quality requirements**: Background noise, microphone quality, and network bandwidth affect speech recognition...",
    "keywords": [
      "speech",
      "audio",
      "quality",
      "requirements",
      "require",
      "users",
      "considerations",
      "implementing",
      "capabilities",
      "application",
      "evaluate",
      "factors",
      "stt",
      "pronunciation",
      "listening",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "recording",
      "microphone",
      "speech-to-text"
    ]
  },
  {
    "id": 60,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Audio capture: Convert analog audio to digital",
    "content": "Speech recognition begins when a microphone converts sound waves into a digital signal. The system samples the analog audio thousands of times per second—typically 16,000 samples per second (16 kHz) for speech applications—and stores each measurement as a numeric value.\n\n![Diagram of an audio waveform.](../media/wave-form-spectogram.png)\n\n\n\nBefore moving to the next stage, the system often applies basic filters to remove hums, clicks, or other background noise that could confuse the model.",
    "summary": "Speech recognition begins when a microphone converts sound waves into a digital signal. The system samples the analog audio thousands of times per second—typically 16,000 samples per second (16 kHz) f...",
    "keywords": [
      "audio",
      "analog",
      "digital",
      "speech",
      "system",
      "samples",
      "capture",
      "convert",
      "recognition",
      "begins",
      "microphone",
      "converts",
      "stt",
      "pronunciation",
      "template",
      "listening",
      "field",
      "extraction",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "structured-data",
      "recording",
      "speech-to-text"
    ]
  },
  {
    "id": 61,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Pre-processing: Extract meaningful features",
    "content": "Raw audio samples contain too much information for efficient pattern recognition. Pre-processing transforms the waveform into a compact representation that highlights speech characteristics while discarding irrelevant details like absolute volume.",
    "summary": "Raw audio samples contain too much information for efficient pattern recognition. Pre-processing transforms the waveform into a compact representation that highlights speech characteristics while disc...",
    "keywords": [
      "pre-processing",
      "extract",
      "meaningful",
      "features",
      "audio",
      "samples",
      "contain",
      "information",
      "efficient",
      "pattern",
      "recognition",
      "transforms",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "structured-data",
      "accent",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 62,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Mel-Frequency Cepstral Coefficients (MFCCs)",
    "content": "MFCC is the most common feature extraction technique in speech recognition. It mimics how the human ear perceives sound by emphasizing frequencies where speech energy concentrates and compressing less important ranges.",
    "summary": "MFCC is the most common feature extraction technique in speech recognition. It mimics how the human ear perceives sound by emphasizing frequencies where speech energy concentrates and compressing less...",
    "keywords": [
      "speech",
      "mel-frequency",
      "cepstral",
      "coefficients",
      "mfccs",
      "mfcc",
      "common",
      "feature",
      "extraction",
      "technique",
      "recognition",
      "mimics",
      "stt",
      "pronunciation",
      "listening",
      "audio-features",
      "sound-analysis",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "transcription",
      "accent",
      "voice-input",
      "signal-processing",
      "recording",
      "microphone",
      "speech-to-text"
    ]
  },
  {
    "id": 63,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "How MFCC works:",
    "content": "1. **Divide audio into frames:** Split the signal into overlapping 20–30 millisecond windows.\n2. **Apply Fourier transform:** Convert each frame from time domain to frequency domain, revealing which pitches are present.\n3. **Map to Mel scale:** Adjust frequency bins to match human hearing sensitivity—we distinguish low pitches better than high ones.\n4. **Extract coefficients:** Compute a small set of numbers (often 13 coefficients) that summarize the spectral shape of each frame.\n\n![Diagram of a",
    "summary": "1. **Divide audio into frames:** Split the signal into overlapping 20–30 millisecond windows.\n2. **Apply Fourier transform:** Convert each frame from time domain to frequency domain, revealing which p...",
    "keywords": [
      "frame",
      "mfcc",
      "coefficients",
      "feature",
      "vectors",
      "audio",
      "domain",
      "frequency",
      "pitches",
      "divide",
      "frames",
      "split",
      "pronunciation",
      "listening",
      "audio-features",
      "sound-analysis",
      "field",
      "extraction",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "signal-processing",
      "accent",
      "structured-data",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 64,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Acoustic modeling: Recognize phonemes",
    "content": "Acoustic models learn the relationship between audio features and **phonemes**—the smallest units of sound that distinguish words. English uses about 44 phonemes; for example, the word \"cat\" comprises three phonemes: /k/, /æ/, and /t/.",
    "summary": "Acoustic models learn the relationship between audio features and **phonemes**—the smallest units of sound that distinguish words. English uses about 44 phonemes; for example, the word \"cat\" comprises...",
    "keywords": [
      "phonemes",
      "acoustic",
      "modeling",
      "recognize",
      "models",
      "learn",
      "relationship",
      "audio",
      "features",
      "smallest",
      "units",
      "sound",
      "pronunciation",
      "listening",
      "sounds",
      "voice",
      "speaking",
      "dictation",
      "speech-units",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 65,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "From features to phonemes",
    "content": "Modern acoustic models use **transformer architectures**, a type of deep learning network that excels at sequence tasks. The transformer processes the MFCC feature vectors and predicts which phoneme is most likely at each moment in time.\n\nTransformer models achieve effective phoneme prediction through: \n\n- **Attention mechanism:** The model examines surrounding frames to resolve ambiguity. For example, the phoneme /t/ sounds different at the start of \"top\" versus the end of \"bat.\"\n- **Parallel p",
    "summary": "Modern acoustic models use **transformer architectures**, a type of deep learning network that excels at sequence tasks. The transformer processes the MFCC feature vectors and predicts which phoneme i...",
    "keywords": [
      "phoneme",
      "phonemes",
      "models",
      "transformer",
      "acoustic",
      "network",
      "frames",
      "frame",
      "features",
      "modern",
      "architectures",
      "type",
      "audio-features",
      "extraction",
      "microphone",
      "rnn",
      "field",
      "audio",
      "speaking",
      "dictation",
      "signal-processing",
      "recording",
      "template",
      "sound-analysis",
      "cnn",
      "neural-networks",
      "voice",
      "speech-units",
      "pronunciation",
      "listening",
      "sounds",
      "sound",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 66,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Language modeling: Predict word sequences",
    "content": "Phoneme predictions alone don't guarantee accurate transcription. The acoustic model might confuse \"their\" and \"there\" because they share identical phonemes. Language models resolve ambiguity by applying knowledge of vocabulary, grammar, and common word patterns. Some ways in which the model guides word sequence prediction include:\n\n- **Statistical patterns:** The model knows \"The weather is nice\" appears more often in training data than \"The whether is nice.\"\n- **Context awareness:** After hear",
    "summary": "Phoneme predictions alone don't guarantee accurate transcription. The acoustic model might confuse \"their\" and \"there\" because they share identical phonemes. Language models resolve ambiguity by apply...",
    "keywords": [
      "model",
      "language",
      "word",
      "models",
      "patterns",
      "nice",
      "modeling",
      "predict",
      "sequences",
      "phoneme",
      "predictions",
      "alone",
      "pronunciation",
      "listening",
      "sounds",
      "acoustic",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "speech-units",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 67,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Decoding: Select the best text hypothesis",
    "content": "Decoding algorithms search through millions of possible word sequences to find the transcription that best matches both acoustic and language model predictions. This stage balances two competing goals: staying faithful to the audio signal while producing readable, grammatically correct text.",
    "summary": "Decoding algorithms search through millions of possible word sequences to find the transcription that best matches both acoustic and language model predictions. This stage balances two competing goals...",
    "keywords": [
      "decoding",
      "best",
      "text",
      "select",
      "hypothesis",
      "algorithms",
      "search",
      "millions",
      "possible",
      "word",
      "sequences",
      "find",
      "pronunciation",
      "listening",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 68,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Beam search decoding:",
    "content": "The most common technique, *beam search*, maintains a shortlist (the \"beam\") of top-scoring partial transcriptions as it processes each audio frame. At every step, it extends each hypothesis with the next most likely word, prunes low-scoring paths, and keeps only the best candidates.\n\nFor a three-second utterance, the decoder might evaluate thousands of hypotheses before selecting \"Please send the report by Friday\" over alternatives like \"Please sent the report buy Friday.\"\n\n> [!CAUTION]\n> Decod",
    "summary": "The most common technique, *beam search*, maintains a shortlist (the \"beam\") of top-scoring partial transcriptions as it processes each audio frame. At every step, it extends each hypothesis with the...",
    "keywords": [
      "beam",
      "search",
      "decoding",
      "hypothesis",
      "please",
      "report",
      "friday",
      "common",
      "technique",
      "maintains",
      "shortlist",
      "top-scoring",
      "pronunciation",
      "listening",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 69,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Post-processing: Refine the output",
    "content": "The decoder produces raw text that often requires cleanup before presentation. Post-processing applies formatting rules and corrections to improve readability and accuracy.",
    "summary": "The decoder produces raw text that often requires cleanup before presentation. Post-processing applies formatting rules and corrections to improve readability and accuracy.",
    "keywords": [
      "post-processing",
      "refine",
      "output",
      "decoder",
      "produces",
      "text",
      "requires",
      "cleanup",
      "presentation",
      "applies",
      "formatting",
      "rules",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "structured-data",
      "accent",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 70,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "Common post-processing tasks:",
    "content": "- **Capitalization:** Convert \"hello my name is sam\" to \"Hello my name is Sam.\"\n- **Punctuation restoration:** Add periods, commas, and question marks based on prosody and grammar.\n- **Number formatting:** Change \"one thousand twenty three\" to \"1,023.\"\n- **Profanity filtering:** Mask or remove inappropriate words when required by policy.\n- **Inverse text normalization:** Convert spoken forms like \"three p m\" to \"3 PM.\"\n- **Confidence scoring:** Flag low-confidence words for human review in criti",
    "summary": "- **Capitalization:** Convert \"hello my name is sam\" to \"Hello my name is Sam.\"\n- **Punctuation restoration:** Add periods, commas, and question marks based on prosody and grammar.\n- **Number formatti...",
    "keywords": [
      "convert",
      "hello",
      "name",
      "three",
      "words",
      "confidence",
      "transcription",
      "common",
      "post-processing",
      "tasks",
      "capitalization",
      "punctuation",
      "pronunciation",
      "artificial-intelligence",
      "listening",
      "field",
      "extraction",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "ai",
      "accent",
      "structured-data",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 71,
    "category": "Speech",
    "file": "02-speech-recognition.md",
    "heading": "How the pipeline works together",
    "content": "Each stage builds on the previous one:\n\n1. **Audio capture** provides the raw signal.\n2. **Pre-processing** extracts MFCC features that highlight speech patterns.\n3. **Acoustic modeling** predicts phoneme probabilities using transformer networks.\n4. **Language modeling** applies vocabulary and grammar knowledge.\n5. **Decoding** searches for the best word sequence.\n6. **Post-processing** formats the text for human readers.\n\nBy separating concerns, modern speech recognition systems achieve high ac",
    "summary": "Each stage builds on the previous one:\n\n1. **Audio capture** provides the raw signal.\n2. **Pre-processing** extracts MFCC features that highlight speech patterns.\n3. **Acoustic modeling** predicts pho...",
    "keywords": [
      "stage",
      "audio",
      "capture",
      "speech",
      "acoustic",
      "modeling",
      "language",
      "post-processing",
      "pipeline",
      "together",
      "builds",
      "previous",
      "stt",
      "audio-features",
      "extraction",
      "microphone",
      "field",
      "voice-input",
      "speaking",
      "dictation",
      "signal-processing",
      "recording",
      "speech-to-text",
      "template",
      "sound-analysis",
      "voice",
      "speech-units",
      "transcription",
      "pronunciation",
      "listening",
      "sounds",
      "sound",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 72,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Text normalization: Standardize the text",
    "content": "Text normalization prepares raw text for pronunciation by expanding abbreviations, numbers, and symbols into spoken forms.\n\nConsider the sentence: \"*Dr. Smith ordered 3 items for $25.50 on 12/15/2023.*\"\n\nA normalization system converts it to: \"Doctor Smith ordered three items for twenty-five dollars and fifty cents on December fifteenth, two thousand twenty-three.\"\n\nCommon normalization tasks include:\n\n- Expanding abbreviations (\"Dr.\" becomes \"Doctor\", \"Inc.\" becomes \"Incorporated\")\n- Converting",
    "summary": "Text normalization prepares raw text for pronunciation by expanding abbreviations, numbers, and symbols into spoken forms.\n\nConsider the sentence: \"*Dr. Smith ordered 3 items for $25.50 on 12/15/2023....",
    "keywords": [
      "becomes",
      "text",
      "normalization",
      "symbols",
      "expanding",
      "abbreviations",
      "numbers",
      "smith",
      "ordered",
      "items",
      "2023",
      "system",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "structured-data",
      "accent",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 73,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Linguistic analysis: Map text to phonemes",
    "content": "Linguistic analysis breaks normalized text into *phonemes* (the smallest units of sound) and determines how to pronounce each word. The linguistic analysis stage:\n\n1. Segments text into words and syllables.\n2. Looks up word pronunciations in lexicons (pronunciation dictionaries).\n3. Applies G2P rules or neural models to handle unknown words.\n4. Marks syllable boundaries and identifies stressed syllables.\n5. Determines phonetic context for adjacent sounds.",
    "summary": "Linguistic analysis breaks normalized text into *phonemes* (the smallest units of sound) and determines how to pronounce each word. The linguistic analysis stage:\n\n1. Segments text into words and syll...",
    "keywords": [
      "linguistic",
      "analysis",
      "text",
      "phonemes",
      "determines",
      "word",
      "words",
      "syllables",
      "breaks",
      "normalized",
      "smallest",
      "units",
      "pronunciation",
      "listening",
      "sounds",
      "acoustic",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "speech-units",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 74,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Grapheme-to-phoneme conversion",
    "content": "Grapheme-to-phoneme (G2P) conversion maps written letters (*graphemes*) to pronunciation sounds (*phonemes*). English spelling doesn't reliably indicate pronunciation, so G2P systems use both rules and learned patterns.\n\nFor example:\n\n- The word \"though\" converts to /θoʊ/\n- The word \"through\" converts to /θruː/\n- The word \"cough\" converts to /kɔːf/\n\nEach word contains the letters \"ough\", but the pronunciation differs dramatically.\n\nModern G2P systems use neural networks trained on pronunciation ",
    "summary": "Grapheme-to-phoneme (G2P) conversion maps written letters (*graphemes*) to pronunciation sounds (*phonemes*). English spelling doesn't reliably indicate pronunciation, so G2P systems use both rules an...",
    "keywords": [
      "word",
      "pronunciation",
      "systems",
      "converts",
      "read",
      "grapheme-to-phoneme",
      "conversion",
      "letters",
      "phonemes",
      "spelling",
      "patterns",
      "example",
      "extraction",
      "acoustic",
      "deep-learning",
      "microphone",
      "neurons",
      "field",
      "layers",
      "backpropagation",
      "audio",
      "speaking",
      "dictation",
      "recording",
      "template",
      "voice",
      "speech-units",
      "listening",
      "sounds",
      "sound",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 75,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Prosody generation: Determine pronunciation",
    "content": "Prosody refers to the rhythm, stress, and intonation patterns that make speech sound natural. Prosody generation determines **how** to say words, not just **which sounds** to produce.",
    "summary": "Prosody refers to the rhythm, stress, and intonation patterns that make speech sound natural. Prosody generation determines **how** to say words, not just **which sounds** to produce.",
    "keywords": [
      "prosody",
      "generation",
      "determine",
      "pronunciation",
      "refers",
      "rhythm",
      "stress",
      "intonation",
      "patterns",
      "speech",
      "sound",
      "natural",
      "listening",
      "audio",
      "voice",
      "speaking",
      "dictation",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 76,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Elements of prosody",
    "content": "Prosody encompasses several vocal characteristics:\n\n- **Pitch contours**: Rising or falling pitch patterns that signal questions versus statements\n- **Duration**: How long to hold each sound, creating emphasis or natural rhythm\n- **Intensity**: Volume variations that highlight important words\n- **Pauses**: Breaks between phrases or sentences that aid comprehension\n- **Stress patterns**: Which syllables receive emphasis within words and sentences\n\nProsody has a significant effect on how spoken te",
    "summary": "Prosody encompasses several vocal characteristics:\n\n- **Pitch contours**: Rising or falling pitch patterns that signal questions versus statements\n- **Duration**: How long to hold each sound, creating...",
    "keywords": [
      "cake",
      "prosody",
      "pitch",
      "patterns",
      "emphasis",
      "words",
      "sentences",
      "elements",
      "encompasses",
      "several",
      "vocal",
      "characteristics",
      "pronunciation",
      "listening",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "accent",
      "recording",
      "microphone"
    ]
  },
  {
    "id": 77,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Transformer-based prosody prediction",
    "content": "Modern speech synthesis systems use transformer neural networks to predict prosody. Transformers excel at understanding context across entire sentences, not just adjacent words.",
    "summary": "Modern speech synthesis systems use transformer neural networks to predict prosody. Transformers excel at understanding context across entire sentences, not just adjacent words.",
    "keywords": [
      "prosody",
      "transformer-based",
      "prediction",
      "modern",
      "speech",
      "synthesis",
      "systems",
      "transformer",
      "neural",
      "networks",
      "predict",
      "transformers",
      "extraction",
      "narration",
      "deep-learning",
      "microphone",
      "neurons",
      "field",
      "layers",
      "backpropagation",
      "audio",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "pronunciation",
      "listening",
      "tts",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 78,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The prosody generation process:",
    "content": "1. **Input encoding**: The transformer receives the phoneme sequence with linguistic features (punctuation, part of speech, sentence structure)\n2. **Contextual analysis**: Self-attention mechanisms identify relationships between words (for example, which noun a pronoun references, where sentence boundaries fall)\n3. **Prosody prediction**: The model outputs predicted values for pitch, duration, and energy at each phoneme\n4. **Style factors**: The system considers speaking style (neutral, expressi",
    "summary": "1. **Input encoding**: The transformer receives the phoneme sequence with linguistic features (punctuation, part of speech, sentence structure)\n2. **Contextual analysis**: Self-attention mechanisms id...",
    "keywords": [
      "prosody",
      "words",
      "pitch",
      "phoneme",
      "speech",
      "sentence",
      "model",
      "style",
      "generation",
      "process",
      "input",
      "encoding",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "sounds",
      "acoustic",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "speech-units",
      "accent",
      "structured-data",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 79,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Factors influencing prosody choices:",
    "content": "- **Syntax**: Clause boundaries indicate where to pause\n- **Semantics**: Important concepts receive emphasis\n- **Discourse context**: Contrasting information or answers to questions may carry extra stress\n- **Speaker identity**: Each voice has characteristic pitch range and speaking rate\n- **Emotional tone**: Excitement, concern, or neutrality shape prosodic patterns\n\nThe prosody predictions create a target specification: \"Produce the phoneme /æ/ at 180 Hz for 80 milliseconds with moderate inten",
    "summary": "- **Syntax**: Clause boundaries indicate where to pause\n- **Semantics**: Important concepts receive emphasis\n- **Discourse context**: Contrasting information or answers to questions may carry extra st...",
    "keywords": [
      "prosody",
      "pause",
      "important",
      "phoneme",
      "milliseconds",
      "factors",
      "influencing",
      "choices",
      "syntax",
      "clause",
      "boundaries",
      "indicate",
      "pronunciation",
      "listening",
      "field",
      "extraction",
      "sounds",
      "acoustic",
      "audio",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "speech-units",
      "accent",
      "structured-data",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 80,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Speech synthesis: Generate audio",
    "content": "Speech synthesis generates the final audio waveform based on the phoneme sequence and prosody specifications.",
    "summary": "Speech synthesis generates the final audio waveform based on the phoneme sequence and prosody specifications.",
    "keywords": [
      "speech",
      "synthesis",
      "audio",
      "generate",
      "generates",
      "final",
      "waveform",
      "based",
      "phoneme",
      "sequence",
      "prosody",
      "specifications",
      "extraction",
      "narration",
      "acoustic",
      "microphone",
      "field",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "speech-units",
      "pronunciation",
      "listening",
      "tts",
      "sounds",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 81,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "Waveform generation approaches",
    "content": "Modern systems use neural vocoders—deep learning models that generate audio samples directly. Popular vocoder architectures include WaveNet, WaveGlow, and HiFi-GAN.",
    "summary": "Modern systems use neural vocoders—deep learning models that generate audio samples directly. Popular vocoder architectures include WaveNet, WaveGlow, and HiFi-GAN.",
    "keywords": [
      "waveform",
      "generation",
      "approaches",
      "modern",
      "systems",
      "neural",
      "vocoders",
      "deep",
      "learning",
      "models",
      "generate",
      "audio",
      "pronunciation",
      "listening",
      "cnn",
      "rnn",
      "field",
      "transformer",
      "neural-networks",
      "extraction",
      "voice",
      "sound",
      "speaking",
      "dictation",
      "structured-data",
      "accent",
      "recording",
      "microphone",
      "template"
    ]
  },
  {
    "id": 82,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The synthesis process:",
    "content": "1. **Acoustic feature generation**: An acoustic model (often a transformer) converts phonemes and prosody targets into mel-spectrograms—visual representations of sound frequencies over time\n2. **Vocoding**: The neural vocoder converts mel-spectrograms into raw audio waveforms (sequences of amplitude values at 16,000-48,000 samples per second)\n3. **Post-processing**: The system applies filtering, normalization, or audio effects to match target output specifications\n\n\n\nThe vocoder essentially perf",
    "summary": "1. **Acoustic feature generation**: An acoustic model (often a transformer) converts phonemes and prosody targets into mel-spectrograms—visual representations of sound frequencies over time\n2. **Vocod...",
    "keywords": [
      "converts",
      "audio",
      "vocoder",
      "acoustic",
      "mel-spectrograms",
      "representations",
      "speech",
      "recognition",
      "synthesis",
      "process",
      "feature",
      "generation",
      "stt",
      "extraction",
      "microphone",
      "field",
      "voice-input",
      "speaking",
      "dictation",
      "recording",
      "template",
      "speech-to-text",
      "voice",
      "speech-units",
      "transcription",
      "pronunciation",
      "listening",
      "sounds",
      "sound",
      "structured-data",
      "accent"
    ]
  },
  {
    "id": 83,
    "category": "Speech",
    "file": "03-speech-synthesis.md",
    "heading": "The complete pipeline in action",
    "content": "When you request speech synthesis for \"Dr. Chen's appointment is at 3:00 PM\":\n\n1. **Text normalization** expands it to \"Doctor Chen's appointment is at three o'clock P M\"\n2. **Linguistic analysis** converts it to phonemes: /ˈdɑktər ˈtʃɛnz əˈpɔɪntmənt ɪz æt θri əˈklɑk pi ɛm/\n3. **Prosody generation** predicts pitch rising slightly on \"appointment\", a pause after \"is\", and emphasis on \"three\"\n4. **Speech synthesis** generates an audio waveform matching those specifications\n\nThe entire process typi",
    "summary": "When you request speech synthesis for \"Dr. Chen's appointment is at 3:00 PM\":\n\n1. **Text normalization** expands it to \"Doctor Chen's appointment is at three o'clock P M\"\n2. **Linguistic analysis** co...",
    "keywords": [
      "appointment",
      "speech",
      "synthesis",
      "chen",
      "three",
      "complete",
      "pipeline",
      "action",
      "request",
      "text",
      "normalization",
      "expands",
      "extraction",
      "narration",
      "acoustic",
      "microphone",
      "field",
      "audio",
      "speaking",
      "dictation",
      "voice-generation",
      "recording",
      "template",
      "voice",
      "speech-units",
      "pronunciation",
      "listening",
      "tts",
      "sounds",
      "reading-aloud",
      "sound",
      "structured-data",
      "accent",
      "text-to-speech"
    ]
  },
  {
    "id": 84,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Image classification",
    "content": "One of the oldest computer vision solutions is a technique called *image classification*, in which a model that has been trained with a large number of images is used to predict a text label based on an image's contents.\n\nFor example, suppose a grocery store wants to implement smart checkout system that identifies produce automatically. For example, the customer could place fruits or vegetables on a scale at the checkout, and an AI application connected to a camera could automatically identify t",
    "summary": "One of the oldest computer vision solutions is a technique called *image classification*, in which a model that has been trained with a large number of images is used to predict a text label based on...",
    "keywords": [
      "image",
      "model",
      "classification",
      "trained",
      "images",
      "predict",
      "based",
      "example",
      "checkout",
      "produce",
      "automatically",
      "apple",
      "camera",
      "artificial-intelligence",
      "seeing",
      "categorization",
      "prediction",
      "picture",
      "photo",
      "detection",
      "sorting",
      "visual",
      "labeling",
      "tagging",
      "ai",
      "image-recognition",
      "recognition",
      "sight",
      "identification"
    ]
  },
  {
    "id": 85,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Object detection",
    "content": "Suppose the grocery store wants a more sophisticated system, in which the checkout can scan multiple items on the checkout and identify each of them. A common approach to this type of problem is called \"object detection\". Object detection models examine multiple regions in an image to find individual objects and their locations. The resulting prediction from the model includes which objects were detected, and the specific regions of the image in which they appear - indicated by the coordinates o",
    "summary": "Suppose the grocery store wants a more sophisticated system, in which the checkout can scan multiple items on the checkout and identify each of them. A common approach to this type of problem is calle...",
    "keywords": [
      "object",
      "detection",
      "checkout",
      "multiple",
      "regions",
      "image",
      "objects",
      "bounding",
      "suppose",
      "grocery",
      "store",
      "sophisticated",
      "finding-objects",
      "camera",
      "localization",
      "seeing",
      "bounding-box",
      "item-detection",
      "picture",
      "photo",
      "visual",
      "recognition",
      "sight",
      "identification"
    ]
  },
  {
    "id": 86,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Semantic segmentation",
    "content": "Another, more sophisticated way to detect objects in an image, is called \"semantic segmentation\". In this approach, a model is trained to find objects, and classify individual pixels in the image based on the object to which they belong. The result of this process is a much more precise prediction of the location of objects in the image.\n\n![Photograph of an orange, apple, and banana with overlaid masks.](../media/semantic-segmentation.png)",
    "summary": "Another, more sophisticated way to detect objects in an image, is called \"semantic segmentation\". In this approach, a model is trained to find objects, and classify individual pixels in the image base...",
    "keywords": [
      "objects",
      "image",
      "semantic",
      "segmentation",
      "another",
      "sophisticated",
      "detect",
      "called",
      "approach",
      "model",
      "trained",
      "find",
      "camera",
      "region-detection",
      "seeing",
      "picture",
      "photo",
      "detection",
      "visual",
      "pixel-classification",
      "recognition",
      "masking",
      "sight",
      "identification"
    ]
  },
  {
    "id": 87,
    "category": "Computer Vision",
    "file": "01-overview.md",
    "heading": "Contextual image analysis",
    "content": "The latest *multimodal* computer vision models are trained to find contextual relationships between objects in images and the text that describes them. The result is an ability to semantically interpret an image to determine what objects and activities it depicts; and generate appropriate descriptions or suggest relevant tags.\n\n![Photograph of a person eating an apple.](../media/image-analysis.png)\n\n***A person eating an apple.***",
    "summary": "The latest *multimodal* computer vision models are trained to find contextual relationships between objects in images and the text that describes them. The result is an ability to semantically interpr...",
    "keywords": [
      "contextual",
      "image",
      "objects",
      "person",
      "eating",
      "apple",
      "analysis",
      "latest",
      "multimodal",
      "computer",
      "vision",
      "models",
      "camera",
      "seeing",
      "picture",
      "photo",
      "detection",
      "visual",
      "recognition",
      "sight",
      "identification"
    ]
  },
  {
    "id": 88,
    "category": "Computer Vision",
    "file": "02-understand-image-processing.md",
    "heading": "Filters",
    "content": "A common way to perform image processing tasks is to apply *filters* that modify the pixel values of the image to create a visual effect. A filter is defined by one or more arrays of pixel values, called filter *kernels*. For example, you could define filter with a 3x3 kernel as shown in this example:\n\n```\n-1 -1 -1\n-1  8 -1\n-1 -1 -1\n```\n\nThe kernel is then *convolved* across the image, calculating a weighted sum for each 3x3 patch of pixels and assigning the result to a new image. It's easier to",
    "summary": "A common way to perform image processing tasks is to apply *filters* that modify the pixel values of the image to create a visual effect. A filter is defined by one or more arrays of pixel values, cal...",
    "keywords": [
      "filter",
      "image",
      "values",
      "pixel",
      "example",
      "kernel",
      "value",
      "convolved",
      "array",
      "effect",
      "result",
      "-255",
      "camera",
      "seeing",
      "field",
      "picture",
      "photo",
      "extraction",
      "detection",
      "visual",
      "structured-data",
      "recognition",
      "sight",
      "identification",
      "template"
    ]
  },
  {
    "id": 89,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Semantic modeling for language - Transformers",
    "content": "Transformers work by processing huge volumes of data, and encoding language *tokens* (representing individual words or phrases) as vector-based *embeddings* (arrays of numeric values). A technique called *attention* is used to assign embedding values that reflect different aspects of how each token is used in the context of other tokens. You can think of the embeddings as vectors in multidimensional space, in which each dimension embeds a linguistic attribute of a token based on its context in t",
    "summary": "Transformers work by processing huge volumes of data, and encoding language *tokens* (representing individual words or phrases) as vector-based *embeddings* (arrays of numeric values). A technique cal...",
    "keywords": [
      "tokens",
      "language",
      "semantic",
      "token",
      "vectors",
      "similar",
      "transformers",
      "words",
      "embeddings",
      "values",
      "context",
      "space",
      "camera",
      "seeing",
      "field",
      "word",
      "picture",
      "photo",
      "vocabulary",
      "detection",
      "tokenization",
      "visual",
      "extraction",
      "structured-data",
      "recognition",
      "sight",
      "embedding",
      "identification",
      "template"
    ]
  },
  {
    "id": 90,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Semantic model for images - Vision transformers",
    "content": "The success of transformers as a way to build language models has led AI researchers to consider whether the same approach would be effective for image data. The result is the development of *vision transformer* (ViT) models, in which a model is trained using a large volume of images. Instead of encoding text-based tokens, the transformer extracts *patches* of pixel values from the image, and generates a linear vector from the pixel values.\n\n![Diagram of a photo with patches assigned to vectors.",
    "summary": "The success of transformers as a way to build language models has led AI researchers to consider whether the same approach would be effective for image data. The result is the development of *vision t...",
    "keywords": [
      "visual",
      "features",
      "models",
      "model",
      "images",
      "vision",
      "language",
      "result",
      "patches",
      "values",
      "vectors",
      "semantic",
      "camera",
      "vocabulary",
      "extraction",
      "detection",
      "artificial-intelligence",
      "field",
      "photo",
      "template",
      "seeing",
      "word",
      "picture",
      "tokenization",
      "identification",
      "ai",
      "structured-data",
      "recognition",
      "sight",
      "embedding"
    ]
  },
  {
    "id": 91,
    "category": "Computer Vision",
    "file": "04-modern-vision-models.md",
    "heading": "Bringing it all together - Multimodal models",
    "content": "A language transformer creates embeddings that define a linguistic vocabulary that encode semantic relationships between words. A vision transformer creates a visual vocabulary that does the same for visual features. When the training data includes images with associated text descriptions, we can combine the encoders from both of these transformers in a *multimodal* model; and use a technique called *cross-model attention* to define a unified spatial representation of the embeddings, like this.\n",
    "summary": "A language transformer creates embeddings that define a linguistic vocabulary that encode semantic relationships between words. A vision transformer creates a visual vocabulary that does the same for...",
    "keywords": [
      "language",
      "embeddings",
      "visual",
      "model",
      "vision",
      "features",
      "multimodal",
      "transformer",
      "creates",
      "define",
      "vocabulary",
      "semantic",
      "camera",
      "seeing",
      "field",
      "picture",
      "photo",
      "extraction",
      "detection",
      "structured-data",
      "recognition",
      "sight",
      "identification",
      "template"
    ]
  },
  {
    "id": 92,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Financial document processing",
    "content": "**Invoice processing** solutions can analyze invoices to extract:\n\n- **Vendor information**: Company names, addresses, and contact details.\n- **Transaction details**: Invoice numbers, dates, and payment terms.\n- **Line items**: Product descriptions, quantities, unit prices, and totals.\n- **Tax information**: Tax rates, amounts, and exempt items.\n\n**Receipt processing** solutions might need to read receipts to extract:\n\n- **Merchant details**: Store names, locations, and transaction IDs.\n- **Purc",
    "summary": "**Invoice processing** solutions can analyze invoices to extract:\n\n- **Vendor information**: Company names, addresses, and contact details.\n- **Transaction details**: Invoice numbers, dates, and payme...",
    "keywords": [
      "information",
      "details",
      "processing",
      "extract",
      "transaction",
      "payment",
      "items",
      "financial",
      "invoice",
      "solutions",
      "names",
      "numbers",
      "receipt",
      "automation",
      "scan",
      "template",
      "field",
      "financial-document",
      "parsing",
      "extraction",
      "workflow",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "billing",
      "document"
    ]
  },
  {
    "id": 93,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Legal and compliance documents",
    "content": "**Contract processing** solutions can be used to extract:\n\n- **Party information**: Contracting parties, signatories, and witnesses.\n- **Terms and conditions**: Effective dates, renewal terms, and termination clauses.\n- **Financial terms**: Payment schedules, penalties, and insurance requirements.\n\n**Regulatory forms** that may need to be processed include:\n\n- **Tax documents**: W-2s, 1099s, and other tax forms.\n- **Insurance forms**: Policy numbers, claim amounts, and incident details.\n- **Gove",
    "summary": "**Contract processing** solutions can be used to extract:\n\n- **Party information**: Contracting parties, signatories, and witnesses.\n- **Terms and conditions**: Effective dates, renewal terms, and ter...",
    "keywords": [
      "forms",
      "terms",
      "documents",
      "insurance",
      "requirements",
      "legal",
      "compliance",
      "contract",
      "processing",
      "solutions",
      "extract",
      "party",
      "automation",
      "scan",
      "template",
      "field",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 94,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Healthcare documentation",
    "content": "**Medical records** can be processed to retrieve:\n\n- **Patient information**: Demographics, medical record numbers, and insurance details.\n- **Clinical data**: Diagnoses, treatments, medication lists, and vital signs.\n- **Administrative data**: Appointment schedules, billing codes, and provider information.",
    "summary": "**Medical records** can be processed to retrieve:\n\n- **Patient information**: Demographics, medical record numbers, and insurance details.\n- **Clinical data**: Diagnoses, treatments, medication lists,...",
    "keywords": [
      "medical",
      "information",
      "data",
      "healthcare",
      "documentation",
      "records",
      "processed",
      "retrieve",
      "patient",
      "demographics",
      "record",
      "numbers",
      "automation",
      "scan",
      "template",
      "field",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 95,
    "category": "Information Extraction",
    "file": "01-introduction.md",
    "heading": "Supply chain and logistics",
    "content": "**Shipping documents** often contain vital details such as:\n\n- **Shipment details**: Tracking numbers, weights, and dimensions.\n- **Address information**: Sender and recipient details, and delivery instructions.\n- **Customs documentation**: Commodity codes, values, and country of origin.\n\n**Purchase Orders** are commonly processed to extract:\n\n- **Vendor information**: Supplier details and contact information.\n- **Product specifications**: Item codes, descriptions, and quantities.\n- **Delivery r",
    "summary": "**Shipping documents** often contain vital details such as:\n\n- **Shipment details**: Tracking numbers, weights, and dimensions.\n- **Address information**: Sender and recipient details, and delivery in...",
    "keywords": [
      "details",
      "information",
      "delivery",
      "instructions",
      "codes",
      "extract",
      "supply",
      "chain",
      "logistics",
      "shipping",
      "documents",
      "contain",
      "automation",
      "scan",
      "artificial-intelligence",
      "template",
      "field",
      "extraction",
      "parsing",
      "ai",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 96,
    "category": "Information Extraction",
    "file": "02-overview.md",
    "heading": "Choosing the right approach",
    "content": "When planning an information extraction solution, it's important to consider the requirements and constraints that the system must address. Some key considerations include:\n\n- **Document characteristics**. The documents from which you need to extract data are the basis of the whole solution. Consider factors like:\n    - **Layout consistency**: Standardized forms favor template-based approaches, while a need to process multiple formats and layouts may require a more complex machine learning based",
    "summary": "When planning an information extraction solution, it's important to consider the requirements and constraints that the system must address. Some key considerations include:\n\n- **Document characteristi...",
    "keywords": [
      "requirements",
      "solution",
      "data",
      "processing",
      "consider",
      "require",
      "learning",
      "information",
      "extraction",
      "constraints",
      "system",
      "documents",
      "automation",
      "scan",
      "artificial-intelligence",
      "cnn",
      "rnn",
      "template",
      "field",
      "transformer",
      "parsing",
      "neural-networks",
      "ai",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 97,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "The OCR pipeline: A step-by-step process",
    "content": "The OCR pipeline consists of five essential stages that work together to transform visual information into text data.\n\n![Diagram of the OCR pipeline.](../media/optical-character-recognition.png)\n\nThe stages in the OCR process are:\n\n1. Image acquisition and input.\n1. Preprocessing and image enhancement.\n1. Text region detection.\n1. Character recognition and classification.\n1. Output generation and post-processing.\n\nLet's examine each stage in more depth.",
    "summary": "The OCR pipeline consists of five essential stages that work together to transform visual information into text data.\n\n![Diagram of the OCR pipeline.](../media/optical-character-recognition.png)\n\nThe...",
    "keywords": [
      "pipeline",
      "process",
      "stages",
      "text",
      "image",
      "step-by-step",
      "consists",
      "five",
      "essential",
      "together",
      "transform",
      "visual",
      "categorization",
      "extraction",
      "labeling",
      "reading",
      "document",
      "automation",
      "field",
      "parsing",
      "document-scanning",
      "sorting",
      "form",
      "understanding",
      "character-recognition",
      "template",
      "text-extraction",
      "prediction",
      "scan",
      "reading-text",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 98,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 1: Image acquisition and input",
    "content": "The pipeline begins when an image containing text enters the system. This could be:\n\n- A photograph taken with a smartphone camera.\n- A scanned document from a flatbed or document scanner.\n- A frame extracted from a video stream.\n- A PDF page rendered as an image.",
    "summary": "The pipeline begins when an image containing text enters the system. This could be:\n\n- A photograph taken with a smartphone camera.\n- A scanned document from a flatbed or document scanner.\n- A frame e...",
    "keywords": [
      "image",
      "document",
      "stage",
      "acquisition",
      "input",
      "pipeline",
      "begins",
      "containing",
      "text",
      "enters",
      "system",
      "photograph",
      "automation",
      "scan",
      "parsing",
      "form",
      "understanding",
      "digitization",
      "reading"
    ]
  },
  {
    "id": 99,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 2: Preprocessing and image enhancement",
    "content": "Before text detection begins, the following techniques are used to optimize the image for better recognition accuracy:\n\n- **Noise reduction** removes visual artifacts, dust spots, and scanning imperfections that could interfere with text detection. The specific techniques used to perform noise reduction include:\n\n    - **Filtering and image processing algorithms**: Gaussian filters, median filters, and morphological operations.\n    - **Machine learning models**: Denoising autoencoders and convol",
    "summary": "Before text detection begins, the following techniques are used to optimize the image for better recognition accuracy:\n\n- **Noise reduction** removes visual artifacts, dust spots, and scanning imperfe...",
    "keywords": [
      "image",
      "text",
      "techniques",
      "models",
      "detection",
      "algorithms",
      "learning",
      "networks",
      "document",
      "correction",
      "resolution",
      "enhancement",
      "forecasting",
      "extraction",
      "deep-learning",
      "estimation",
      "reading",
      "automation",
      "neurons",
      "rnn",
      "field",
      "transformer",
      "parsing",
      "layers",
      "backpropagation",
      "form",
      "understanding",
      "template",
      "cnn",
      "prediction",
      "continuous",
      "neural-networks",
      "scan",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 100,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 3: Text region detection",
    "content": "The system analyzes the preprocessed image to identify areas that contain text by using the following techniques:\n\n- **Layout analysis** distinguishes between text regions, images, graphics, and white space areas. Techniques for layout analysis include:\n\n    - **Traditional approaches**: Connected component analysis, run-length encoding, and projection-based segmentation.\n    - **Deep learning models**: Semantic segmentation networks like U-Net, Mask R-CNN, and specialized document layout analys",
    "summary": "The system analyzes the preprocessed image to identify areas that contain text by using the following techniques:\n\n- **Layout analysis** distinguishes between text regions, images, graphics, and white...",
    "keywords": [
      "text",
      "models",
      "analysis",
      "networks",
      "layout",
      "approaches",
      "learning",
      "document",
      "spatial",
      "neural",
      "region",
      "areas",
      "search",
      "categorization",
      "extraction",
      "labeling",
      "pixel-classification",
      "masking",
      "patterns",
      "deep-learning",
      "reading",
      "region-detection",
      "segmentation",
      "automation",
      "neurons",
      "rnn",
      "field",
      "transformer",
      "parsing",
      "sorting",
      "layers",
      "backpropagation",
      "unsupervised",
      "form",
      "understanding",
      "context",
      "template",
      "cnn",
      "prediction",
      "grouping",
      "neural-networks",
      "grounding",
      "scan",
      "retrieval",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 101,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 4: Character recognition and classification",
    "content": "This is the core of the OCR process where individual characters are identified:\n\n- **Feature extraction**: Analyzes the shape, size, and distinctive characteristics of each character or symbol.\n    - **Traditional methods**: Statistical features like moments, Fourier descriptors, and structural features (loops, endpoints, intersections)\n    - **Deep learning approaches**: Convolutional neural networks that automatically learn discriminative features from raw pixel data\n\n- **Pattern matching**: C",
    "summary": "This is the core of the OCR process where individual characters are identified:\n\n- **Feature extraction**: Analyzes the shape, size, and distinctive characteristics of each character or symbol.\n    -...",
    "keywords": [
      "models",
      "character",
      "features",
      "neural",
      "recognition",
      "statistical",
      "networks",
      "language",
      "probability",
      "predictions",
      "classification",
      "characters",
      "categorization",
      "extraction",
      "labeling",
      "deep-learning",
      "reading",
      "document",
      "automation",
      "neurons",
      "rnn",
      "field",
      "transformer",
      "parsing",
      "document-scanning",
      "sorting",
      "layers",
      "backpropagation",
      "form",
      "understanding",
      "character-recognition",
      "text-extraction",
      "template",
      "cnn",
      "prediction",
      "neural-networks",
      "scan",
      "reading-text",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 102,
    "category": "Information Extraction",
    "file": "03-vision-extraction.md",
    "heading": "Stage 5: Output generation and post-processing",
    "content": "The final stage converts recognition results into usable text data:\n\n- **Text compilation**: Assembles individual character recognitions into complete words and sentences.\n    - **Rule-based assembly**: Deterministic algorithms that combine character predictions using spatial proximity and confidence thresholds.\n    - **Sequence models**: Recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks that model text as sequential data.\n    - **Attention-based models**: Transformer a",
    "summary": "The final stage converts recognition results into usable text data:\n\n- **Text compilation**: Assembles individual character recognitions into complete words and sentences.\n    - **Rule-based assembly*...",
    "keywords": [
      "models",
      "text",
      "validation",
      "neural",
      "networks",
      "data",
      "spatial",
      "document",
      "coordinates",
      "stage",
      "recognition",
      "character",
      "search",
      "forecasting",
      "extraction",
      "deep-learning",
      "estimation",
      "reading",
      "automation",
      "artificial-intelligence",
      "neurons",
      "field",
      "parsing",
      "document-scanning",
      "layers",
      "backpropagation",
      "form",
      "understanding",
      "context",
      "character-recognition",
      "text-extraction",
      "template",
      "prediction",
      "continuous",
      "grounding",
      "scan",
      "retrieval",
      "reading-text",
      "ai",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 103,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "The field extraction pipeline",
    "content": "Field extraction follows a systematic pipeline that transforms OCR output into structured data.\n\n![Diagram of the field extraction pipeline.](../media/field-extraction.png)\n\nThe stages in the field extraction process are:\n\n1. OCR output ingestion.\n1. Field detection and candidate identification.\n1. Field mapping and association.\n1. Data normalization and standardization.\n1. Integration with business processes and systems.\n\nLet's explore these stages in more detail.",
    "summary": "Field extraction follows a systematic pipeline that transforms OCR output into structured data.\n\n![Diagram of the field extraction pipeline.](../media/field-extraction.png)\n\nThe stages in the field ex...",
    "keywords": [
      "field",
      "extraction",
      "pipeline",
      "output",
      "data",
      "stages",
      "follows",
      "systematic",
      "transforms",
      "structured",
      "diagram",
      "media",
      "automation",
      "scan",
      "template",
      "structured-output",
      "parsing",
      "character-recognition",
      "document-scanning",
      "reading-text",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "data-extraction",
      "reading",
      "text-extraction",
      "document"
    ]
  },
  {
    "id": 104,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 1: OCR output ingestion",
    "content": "The process begins with the structured output from the OCR pipeline, which can include:\n\n- **Raw text content**: The actual characters and words extracted from the document\n- **Positional metadata**: Bounding box coordinates, page locations, and reading order information\n- **Confidence scores**: OCR engine confidence levels for each text element\n- **Layout information**: Document structure, line breaks, paragraph boundaries",
    "summary": "The process begins with the structured output from the OCR pipeline, which can include:\n\n- **Raw text content**: The actual characters and words extracted from the document\n- **Positional metadata**:...",
    "keywords": [
      "output",
      "text",
      "document",
      "information",
      "confidence",
      "stage",
      "ingestion",
      "process",
      "begins",
      "structured",
      "pipeline",
      "content",
      "search",
      "extraction",
      "reading",
      "automation",
      "field",
      "parsing",
      "document-scanning",
      "form",
      "understanding",
      "context",
      "character-recognition",
      "template",
      "text-extraction",
      "grounding",
      "scan",
      "retrieval",
      "reading-text",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 105,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 2: Field detection and candidate identification",
    "content": "This stage identifies potential field value in the OCR output. There are multiple approaches that can be used, independently or in combination, to determine the likely fields in the OCR results.",
    "summary": "This stage identifies potential field value in the OCR output. There are multiple approaches that can be used, independently or in combination, to determine the likely fields in the OCR results.",
    "keywords": [
      "stage",
      "field",
      "detection",
      "candidate",
      "identification",
      "identifies",
      "potential",
      "value",
      "output",
      "multiple",
      "approaches",
      "independently",
      "automation",
      "scan",
      "parsing",
      "character-recognition",
      "document-scanning",
      "reading-text",
      "form",
      "understanding",
      "digitization",
      "reading",
      "text-extraction",
      "document"
    ]
  },
  {
    "id": 106,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Template-based detection",
    "content": "Templates for field detection rely on rule-based pattern matching. Field identification may be accomplished using techniques such as:\n\n- Predefined document layouts with known field positions and anchor keywords.\n- Searches for label-value pairs like \"Invoice Number:\", \"Date:\", \"Total:\".\n- Regular expressions and string matching algorithms.\n\n**Advantages** of a template-based approach include high accuracy for known document types, fast processing, and explainable results.\n\n**Limitations** of th",
    "summary": "Templates for field detection rely on rule-based pattern matching. Field identification may be accomplished using techniques such as:\n\n- Predefined document layouts with known field positions and anch...",
    "keywords": [
      "field",
      "template-based",
      "detection",
      "matching",
      "document",
      "approach",
      "templates",
      "rely",
      "rule-based",
      "pattern",
      "identification",
      "accomplished",
      "receipt",
      "automation",
      "scan",
      "financial-document",
      "parsing",
      "form",
      "understanding",
      "digitization",
      "reading",
      "billing",
      "payment"
    ]
  },
  {
    "id": 107,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Machine learning-based detection",
    "content": "Instead of hard-coded logic to extract fields based on known names and locations, you can use a corpus of example documents to train a machine learning model that extracts the fields based on learned relationships. *Transformer*-based models in particular are good at applying contextual cues to identify patterns, and so are often the basis of a field detection solution.\n\nTraining approaches for field detection machine learning models include:\n\n- **Supervised learning**: Trained on labeled datase",
    "summary": "Instead of hard-coded logic to extract fields based on known names and locations, you can use a corpus of example documents to train a machine learning model that extracts the fields based on learned...",
    "keywords": [
      "learning",
      "field",
      "machine",
      "detection",
      "model",
      "models",
      "text",
      "fields",
      "based",
      "locations",
      "relationships",
      "patterns",
      "automation",
      "scan",
      "neurons",
      "template",
      "extraction",
      "parsing",
      "layers",
      "backpropagation",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "deep-learning",
      "document"
    ]
  },
  {
    "id": 108,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Generative AI for schema-based extraction",
    "content": "Recent advances in large language models (LLMs) have led to the emergence of generative AI-based field detection techniques, which enable more efficient and effective field detection through:\n\n- **Prompt-based extraction** in which you provide the LLM with document text and a schema definition, and it matches the text to the fields in the schema.\n- **Few-shot learning** in which you can train models with minimal examples to extract custom fields.\n- **Chain-of-thought reasoning** that guides mode",
    "summary": "Recent advances in large language models (LLMs) have led to the emergence of generative AI-based field detection techniques, which enable more efficient and effective field detection through:\n\n- **Pro...",
    "keywords": [
      "models",
      "field",
      "generative",
      "extraction",
      "detection",
      "text",
      "schema",
      "fields",
      "schema-based",
      "recent",
      "advances",
      "language",
      "automation",
      "scan",
      "artificial-intelligence",
      "query",
      "input",
      "parsing",
      "large-language-model",
      "foundation-model",
      "ai",
      "request",
      "form",
      "llm",
      "understanding",
      "instruction",
      "digitization",
      "llms",
      "reading",
      "document"
    ]
  },
  {
    "id": 109,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 3: Field mapping and association",
    "content": "After candidate values are identified, they must be mapped to specific schema fields:",
    "summary": "After candidate values are identified, they must be mapped to specific schema fields:",
    "keywords": [
      "stage",
      "field",
      "mapping",
      "association",
      "candidate",
      "values",
      "identified",
      "mapped",
      "specific",
      "schema",
      "fields",
      "automation",
      "scan",
      "parsing",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 110,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Key-value pairing techniques",
    "content": "In many cases, data fields in a document or form are discrete values that can be mapped to keys - for example, the vendor name, date, and total amount in a receipt or invoice. Common techniques used for key-value pairing include:\n\n- **Proximity analysis**:\n    - **Spatial clustering**: Group nearby text elements using distance algorithms.\n    - **Reading order analysis**: Follow natural text flow to associate labels with values.\n    - **Geometric relationships**: Use alignment, indentation, and ",
    "summary": "In many cases, data fields in a document or form are discrete values that can be mapped to keys - for example, the vendor name, date, and total amount in a receipt or invoice. Common techniques used f...",
    "keywords": [
      "values",
      "text",
      "relationships",
      "key-value",
      "pairing",
      "techniques",
      "analysis",
      "labels",
      "recognition",
      "entity",
      "cases",
      "data",
      "extraction",
      "patterns",
      "reading",
      "document",
      "segmentation",
      "automation",
      "field",
      "parsing",
      "unsupervised",
      "form",
      "understanding",
      "billing",
      "template",
      "receipt",
      "grouping",
      "financial-document",
      "payment",
      "scan",
      "structured-data",
      "digitization"
    ]
  },
  {
    "id": 111,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Table and structured content processing",
    "content": "Some documents include more complex structures of text, such as tables. For example, a receipt or invoice might include a table of line items with columns for the item name, price, and the quantity purchased.\n\nThe presence of a table can be determined using several techniques, including:\n\n- Specialized convolutional neural network (CNN) architectures for table structure recognition.\n- Object detection approaches adapted for table cell identification.\n- Graph-based parsing approaches that model t",
    "summary": "Some documents include more complex structures of text, such as tables. For example, a receipt or invoice might include a table of line items with columns for the item name, price, and the quantity pu...",
    "keywords": [
      "table",
      "cells",
      "field",
      "processing",
      "structures",
      "techniques",
      "structure",
      "detection",
      "approaches",
      "structured",
      "content",
      "documents",
      "deep-learning",
      "reading",
      "document",
      "automation",
      "neurons",
      "item-detection",
      "parsing",
      "layers",
      "backpropagation",
      "form",
      "understanding",
      "billing",
      "receipt",
      "localization",
      "bounding-box",
      "financial-document",
      "data-extraction",
      "structured-output",
      "payment",
      "finding-objects",
      "scan",
      "digitization"
    ]
  },
  {
    "id": 112,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Confidence scoring and validation",
    "content": "Field detection and extraction accuracy depends on many factors, and the algorithms and models used to implement the solution are subject to potential misidentification or value interpretation errors. To account for this, various techniques are employed to evaluate the accuracy of the predicted field values; including:\n\n- **OCR confidence**: Inheriting confidence scores from the underlying text recognition.\n- **Pattern matching confidence**: Scoring based on how well extraction matches expected ",
    "summary": "Field detection and extraction accuracy depends on many factors, and the algorithms and models used to implement the solution are subject to potential misidentification or value interpretation errors....",
    "keywords": [
      "confidence",
      "validation",
      "field",
      "scoring",
      "extraction",
      "accuracy",
      "values",
      "context",
      "verifying",
      "detection",
      "depends",
      "factors",
      "payment",
      "receipt",
      "automation",
      "scan",
      "financial-document",
      "parsing",
      "character-recognition",
      "document-scanning",
      "reading-text",
      "form",
      "understanding",
      "digitization",
      "reading",
      "billing",
      "text-extraction",
      "document"
    ]
  },
  {
    "id": 113,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 4: Data normalization and standardization",
    "content": "Raw extracted values are generally transformed into consistent formats (for example to ensure that all extracted dates are expressed in the same date format) and checked for validity.",
    "summary": "Raw extracted values are generally transformed into consistent formats (for example to ensure that all extracted dates are expressed in the same date format) and checked for validity.",
    "keywords": [
      "extracted",
      "stage",
      "data",
      "normalization",
      "standardization",
      "values",
      "generally",
      "transformed",
      "consistent",
      "formats",
      "example",
      "ensure",
      "automation",
      "scan",
      "template",
      "field",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 114,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Format standardization",
    "content": "Examples of format standardization that can be implemented include:\n\n- **Date normalization**:\n    - **Format detection**: Identify various date formats (MM/DD/YYYY, DD-MM-YYYY, etc.).\n    - **Parsing algorithms**: Convert to standardized ISO formats.\n    - **Ambiguity resolution**: Handle cases where date format is unclear.\n\n- **Currency and numeric processing**:\n    - **Symbol recognition**: Handle different currency symbols and thousand separators.\n    - **Decimal normalization**: Standardize",
    "summary": "Examples of format standardization that can be implemented include:\n\n- **Date normalization**:\n    - **Format detection**: Identify various date formats (MM/DD/YYYY, DD-MM-YYYY, etc.).\n    - **Parsing...",
    "keywords": [
      "format",
      "standardization",
      "date",
      "normalization",
      "convert",
      "handle",
      "different",
      "formats",
      "currency",
      "decimal",
      "examples",
      "implemented",
      "automation",
      "scan",
      "template",
      "field",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 115,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Data Validation and Quality Assurance",
    "content": "As well as formatting the extracted fields, the standardization process enables further validation of the values that have been extracted through techniques like:\n\n- **Rule-based validation**:\n    - **Format checking**: Verify extracted values match expected patterns (phone numbers, email addresses).\n    - **Range validation**: Ensure numeric values fall within reasonable bounds.\n    - **Required field checking**: Confirm all mandatory fields are present.\n\n- **Statistical validation**:\n    - **O",
    "summary": "As well as formatting the extracted fields, the standardization process enables further validation of the values that have been extracted through techniques like:\n\n- **Rule-based validation**:\n    - *...",
    "keywords": [
      "validation",
      "values",
      "extracted",
      "fields",
      "checking",
      "patterns",
      "data",
      "quality",
      "assurance",
      "formatting",
      "standardization",
      "process",
      "automation",
      "scan",
      "template",
      "field",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 116,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Stage 5: Integration with business processes and systems",
    "content": "The final stage of the process usually involves integrating the extracted field values into a business process or system:",
    "summary": "The final stage of the process usually involves integrating the extracted field values into a business process or system:",
    "keywords": [
      "stage",
      "business",
      "process",
      "integration",
      "processes",
      "systems",
      "final",
      "usually",
      "involves",
      "integrating",
      "extracted",
      "field",
      "automation",
      "scan",
      "parsing",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 117,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Schema mapping",
    "content": "The extracted fields may need to be further transformed or reformatted so they align with application schemas used for data ingestion into downstream systems. For example:\n\n- **Database schemas**: Map extracted fields to specific database columns and tables.\n- **API payloads**: Format data for REST API consumption by downstream systems.\n- **Message queues**: Prepare structured messages for asynchronous processing.\n\nThe schema mapping process might involve transformations such as:\n\n- **Field rena",
    "summary": "The extracted fields may need to be further transformed or reformatted so they align with application schemas used for data ingestion into downstream systems. For example:\n\n- **Database schemas**: Map...",
    "keywords": [
      "data",
      "extracted",
      "systems",
      "field",
      "schema",
      "mapping",
      "fields",
      "schemas",
      "downstream",
      "database",
      "target",
      "further",
      "automation",
      "scan",
      "template",
      "extraction",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  },
  {
    "id": 118,
    "category": "Information Extraction",
    "file": "04-form-extraction.md",
    "heading": "Quality metrics and reporting",
    "content": "Another common task after the extraction process has finished is to evaluate and report on the quality of the extracted data. The report can include information such as:\n\n- **Field-level confidence scores**: Individual confidence ratings for each extracted field.\n- **Document-level quality assessment**: Overall extraction success metrics.\n- **Error categorization**: Classify extraction failures by type and cause.",
    "summary": "Another common task after the extraction process has finished is to evaluate and report on the quality of the extracted data. The report can include information such as:\n\n- **Field-level confidence sc...",
    "keywords": [
      "quality",
      "extraction",
      "metrics",
      "report",
      "extracted",
      "confidence",
      "reporting",
      "another",
      "common",
      "task",
      "process",
      "finished",
      "automation",
      "scan",
      "template",
      "field",
      "parsing",
      "structured-data",
      "form",
      "understanding",
      "digitization",
      "reading",
      "document"
    ]
  }
]